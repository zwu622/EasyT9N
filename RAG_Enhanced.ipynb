{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RAG-Enhanced Translation System\n",
    "\n",
    "This notebook builds on the baseline results from `Baseline.ipynb`.\n",
    "\n",
    "**Features:**\n",
    "- **Adaptive Retrieval**: Context-aware glossary and TM retrieval\n",
    "- **Comprehensive Evaluation**: RAG vs baseline comparison\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup & Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RAG-Enhanced Translation System (Clean Rewrite)\n",
      "======================================================================\n",
      "Loaded 76 source segments\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import json\n",
    "import time\n",
    "import random\n",
    "import warnings\n",
    "import hashlib\n",
    "import threading\n",
    "from pathlib import Path\n",
    "from typing import Any, Dict, List, Tuple, Optional\n",
    "\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from dotenv import load_dotenv\n",
    "from tqdm import tqdm\n",
    "\n",
    "# RAG-specific\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import chromadb\n",
    "\n",
    "# OpenAI client\n",
    "from openai import OpenAI\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "pd.set_option(\"display.max_columns\", None)\n",
    "pd.set_option(\"display.width\", None)\n",
    "\n",
    "load_dotenv()\n",
    "print(\"RAG-Enhanced Translation System (Clean Rewrite)\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "def flatten_json_strings(obj: Any, prefix: str = \"\") -> List[Tuple[str, str]]:\n",
    "    out: List[Tuple[str, str]] = []\n",
    "    if isinstance(obj, dict):\n",
    "        for k, v in obj.items():\n",
    "            new_prefix = f\"{prefix}.{k}\" if prefix else k\n",
    "            out.extend(flatten_json_strings(v, new_prefix))\n",
    "    elif isinstance(obj, list):\n",
    "        for i, v in enumerate(obj):\n",
    "            new_prefix = f\"{prefix}[{i}]\"\n",
    "            out.extend(flatten_json_strings(v, new_prefix))\n",
    "    elif isinstance(obj, str) and obj.strip():\n",
    "        out.append((prefix, obj))\n",
    "    return out\n",
    "\n",
    "SRC_FILE = Path(\"data/en.json\")\n",
    "if not SRC_FILE.exists():\n",
    "    raise FileNotFoundError(\"Missing data/en.json\")\n",
    "\n",
    "with open(SRC_FILE, \"r\", encoding=\"utf-8\") as f:\n",
    "    en_json = json.load(f)\n",
    "\n",
    "en_segments: List[Tuple[str, str]] = flatten_json_strings(en_json)\n",
    "print(f\"Loaded {len(en_segments)} source segments\")\n",
    "\n",
    "try:\n",
    "    def _sha1(p): \n",
    "        h = hashlib.sha1()\n",
    "        with open(p, 'rb') as f:\n",
    "            for chunk in iter(lambda: f.read(8192), b''): \n",
    "                h.update(chunk)\n",
    "        return h.hexdigest()\n",
    "    SOURCE_SHA = _sha1(\"data/en.json\")\n",
    "except Exception:\n",
    "    SOURCE_SHA = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load Translation Memory (TM) & Glossary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TM FR: 2 entries\n",
      "TM JA: 2 entries\n",
      "TM IT: 2 entries\n",
      "Glossary FR: 18 mappings\n",
      "Glossary JA: 18 mappings\n",
      "Glossary IT: 18 mappings\n",
      "DNT terms: ['Gel-X', 'NaiLit']\n"
     ]
    }
   ],
   "source": [
    "def load_translation_memory() -> Dict[str, Dict[str, str]]:\n",
    "    tm_dict = {\"fr\": {}, \"ja\": {}, \"it\": {}}\n",
    "    tm_file = Path(\"data/translation_memory.csv\")\n",
    "    if tm_file.exists():\n",
    "        try:\n",
    "            df = pd.read_csv(tm_file)\n",
    "            for _, row in df.iterrows():\n",
    "                lang = row.get(\"tgt_lang\")\n",
    "                src = row.get(\"src_text\")\n",
    "                tgt = row.get(\"tgt_text\")\n",
    "                if lang in tm_dict and isinstance(src, str) and isinstance(tgt, str) and src and tgt:\n",
    "                    tm_dict[lang][src] = tgt\n",
    "            for lang in [\"fr\", \"ja\", \"it\"]:\n",
    "                print(f\"TM {lang.upper()}: {len(tm_dict[lang])} entries\")\n",
    "        except Exception as e:\n",
    "            print(f\"⚠️ Could not load TM: {e}\")\n",
    "    else:\n",
    "        print(\"⚠️ No translation_memory.csv found\")\n",
    "    return tm_dict\n",
    "\n",
    "\n",
    "def load_glossary() -> Tuple[List[str], Dict[str, Dict[str, str]], List[str]]:\n",
    "    glossary_terms: List[str] = []\n",
    "    glossary_map: Dict[str, Dict[str, str]] = {\"fr\": {}, \"ja\": {}, \"it\": {}}\n",
    "    dnt_terms: List[str] = [\"NaiLit\"]  # default DNT brand\n",
    "\n",
    "    gl_file = Path(\"data/glossary.csv\")\n",
    "    if gl_file.exists():\n",
    "        try:\n",
    "            df = pd.read_csv(gl_file)\n",
    "            if \"source_term\" not in df.columns:\n",
    "                raise ValueError(\"glossary.csv must have a 'source_term' column\")\n",
    "            glossary_terms = [t for t in df[\"source_term\"].dropna().astype(str).tolist() if t]\n",
    "            for lang in [\"fr\", \"ja\", \"it\"]:\n",
    "                if lang in df.columns:\n",
    "                    col = df[lang].astype(str)\n",
    "                    glossary_map[lang] = {\n",
    "                        st: tt for st, tt in zip(df[\"source_term\"], col)\n",
    "                        if pd.notna(st) and pd.notna(tt)\n",
    "                    }\n",
    "                    print(f\"Glossary {lang.upper()}: {len(glossary_map[lang])} mappings\")\n",
    "            if \"dnt\" in df.columns:\n",
    "                mask = df[\"dnt\"].astype(str).str.upper() == \"TRUE\"\n",
    "                dnt_terms.extend(df.loc[mask, \"source_term\"].dropna().astype(str).tolist())\n",
    "        except Exception as e:\n",
    "            print(f\"⚠️ Could not load glossary: {e}\")\n",
    "    else:\n",
    "        print(\"⚠️ No glossary.csv found\")\n",
    "\n",
    "    dnt_terms = sorted(list({t for t in dnt_terms if t}))\n",
    "    return glossary_terms, glossary_map, dnt_terms\n",
    "\n",
    "TM_DICT = load_translation_memory()\n",
    "GLOSSARY_TERMS, GLOSSARY_MAP, DNT_TERMS = load_glossary()\n",
    "print(f\"DNT terms: {DNT_TERMS}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Embeddings & Vector DB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading embedding model…\n",
      "Indexing glossary…\n",
      "✅ Glossary ready in Chroma\n"
     ]
    }
   ],
   "source": [
    "print(\"Loading embedding model…\")\n",
    "EMB_MODEL = SentenceTransformer(\"intfloat/multilingual-e5-base\")\n",
    "\n",
    "CHROMA_PATH = \".chroma\"\n",
    "chroma_client = chromadb.PersistentClient(path=CHROMA_PATH)\n",
    "GLOSSARY_COL = chroma_client.get_or_create_collection(\n",
    "    name=\"glossary\", \n",
    "    metadata={\"hnsw:space\": \"cosine\"}\n",
    ")\n",
    "\n",
    "if GLOSSARY_TERMS:\n",
    "    print(\"Indexing glossary…\")\n",
    "    ids_all = [str(i) for i in range(len(GLOSSARY_TERMS))]\n",
    "    try:\n",
    "        existing = set(GLOSSARY_COL.get(ids=ids_all)[\"ids\"])  # may raise if none\n",
    "    except Exception:\n",
    "        existing = set()\n",
    "    to_add = [(i, t) for i, t in enumerate(GLOSSARY_TERMS) if str(i) not in existing]\n",
    "    if to_add:\n",
    "        batch_terms = [t for _, t in to_add]\n",
    "        embs = EMB_MODEL.encode(batch_terms, batch_size=64, normalize_embeddings=True, show_progress_bar=True)\n",
    "        GLOSSARY_COL.add(\n",
    "            ids=[str(i) for i, _ in to_add], \n",
    "            documents=batch_terms, \n",
    "            embeddings=[e.tolist() for e in embs]\n",
    "        )\n",
    "    print(\"✅ Glossary ready in Chroma\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Retrieval & Utility Helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "HTML_TAG = re.compile(r\"</?\\w+(?:\\s+[^>]*?)?>\", re.IGNORECASE)\n",
    "_WORD = re.compile(r\"\\w+\", re.UNICODE)\n",
    "\n",
    "def _normalize_for_retrieval(s: str) -> str:\n",
    "    s = HTML_TAG.sub(\"\", s or \"\")\n",
    "    s = re.sub(r\"\\s+\", \" \", s).strip().lower()\n",
    "    return s\n",
    "\n",
    "def _tokenize(s: str) -> set:\n",
    "    return set(w.lower() for w in _WORD.findall(s or \"\"))\n",
    "\n",
    "def tm_lookup(src_text: str, lang: str) -> Optional[str]:\n",
    "    return TM_DICT.get(lang, {}).get(src_text)\n",
    "\n",
    "def tags_preserved(src: str, tgt: str) -> bool:\n",
    "    return HTML_TAG.findall(src or \"\") == HTML_TAG.findall(tgt or \"\")\n",
    "\n",
    "def retrieve_glossary_terms(\n",
    "    segment_text: str, \n",
    "    top_k: int = 5,\n",
    "    min_score: float = 0.45, \n",
    "    overfetch: int = 24,\n",
    "    require_lex_for_long: bool = True\n",
    ") -> List[str]:\n",
    "    if not (GLOSSARY_COL and GLOSSARY_TERMS):\n",
    "        return []\n",
    "    norm = _normalize_for_retrieval(segment_text)\n",
    "    # E5 query style\n",
    "    q_vec = EMB_MODEL.encode([f\"query: {norm}\"], normalize_embeddings=True)[0].tolist()\n",
    "    try:\n",
    "        res = GLOSSARY_COL.query(query_embeddings=[q_vec], n_results=max(top_k * 3, overfetch))\n",
    "        docs: List[str] = res.get(\"documents\", [[]])[0] if res else []\n",
    "        dists = res.get(\"distances\", [[]])[0] if res else []\n",
    "        sims = [(1.0 - d) if (0.0 <= d <= 2.0) else d for d in dists] if dists else [0.0] * len(docs)\n",
    "    except Exception as e:\n",
    "        print(f\"⚠️ Retrieval error: {e}\")\n",
    "        return []\n",
    "\n",
    "    seg_tokens = _tokenize(norm)\n",
    "\n",
    "    def _lex_boost(term: str) -> float:\n",
    "        t = term.lower()\n",
    "        b = 0.0\n",
    "        if t in norm:\n",
    "            b += 0.12\n",
    "        overlap = len(seg_tokens & _tokenize(t))\n",
    "        if overlap:\n",
    "            b += min(0.05 * overlap, 0.15)\n",
    "        return b\n",
    "\n",
    "    long_text = len(norm) >= 80 if require_lex_for_long else False\n",
    "    scored: Dict[str, float] = {}\n",
    "    for term, base in zip(docs, sims):\n",
    "        lb = _lex_boost(term)\n",
    "        if long_text and lb == 0.0:\n",
    "            continue\n",
    "        score = base + lb\n",
    "        if score >= min_score:\n",
    "            if term not in scored or score > scored[term]:\n",
    "                scored[term] = score\n",
    "\n",
    "    ranked = sorted(scored.items(), key=lambda x: x[1], reverse=True)\n",
    "    return [t for t, _ in ranked[:top_k]]\n",
    "\n",
    "def build_constraints(src_text: str, lang: str, top_k: int = 3) -> List[str]:\n",
    "    if lang not in GLOSSARY_MAP:\n",
    "        return []\n",
    "    terms = retrieve_glossary_terms(src_text, top_k=top_k, min_score=0.45)\n",
    "    pairs: List[str] = []\n",
    "    for en_term in terms:\n",
    "        tgt = GLOSSARY_MAP[lang].get(en_term)\n",
    "        if isinstance(tgt, str) and tgt:\n",
    "            pairs.append(f\"{en_term} → {tgt}\")\n",
    "    return pairs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Open AI client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OPENAI models: {'gpt-4o-mini': {'model': 'gpt-4o-mini'}}\n"
     ]
    }
   ],
   "source": [
    "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\n",
    "if not OPENAI_API_KEY:\n",
    "    raise RuntimeError(\"Missing OPENAI_API_KEY\")\n",
    "\n",
    "OPENAI_CLIENT = OpenAI(api_key=OPENAI_API_KEY)\n",
    "\n",
    "try:\n",
    "    OPENAI_MODELS\n",
    "except NameError:\n",
    "    OPENAI_MODELS = {}\n",
    "\n",
    "try:\n",
    "    BASELINE_MODELS\n",
    "except NameError:\n",
    "    BASELINE_MODELS = {}\n",
    "\n",
    "OPENAI_MODELS[\"gpt-4o-mini\"] = {\n",
    "    \"model\": os.getenv(\"OPENAI_BASELINE_MODEL\", \"gpt-4o-mini\")\n",
    "}\n",
    "BASELINE_MODELS[\"gpt-4o-mini\"] = (\"openai\", OPENAI_MODELS[\"gpt-4o-mini\"])\n",
    "\n",
    "print(\"OPENAI models:\", OPENAI_MODELS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Few-shots & Prompt helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "FEWSHOTS = [\n",
    "    {\n",
    "        \"src\": \"Order <strong>custom designs</strong> or choose from our curated nail art collections\",\n",
    "        \"constraints\": [\"press-on nails → press-on nails\"],\n",
    "        \"tgt_fr\": \"Commandez des <strong>designs personnalisés</strong> ou choisissez parmi nos collections de nail art sélectionnées\",\n",
    "        \"tgt_ja\": \"<strong>カスタムデザイン</strong>を注文するか、厳選されたネイルアートコレクションから選びましょう\",\n",
    "        \"tgt_it\": \"Ordina <strong>design personalizzati</strong> o scegli dalle nostre collezioni curate di nail art\",\n",
    "    },\n",
    "    {\n",
    "        \"src\": \"With <strong>NaiLit</strong>, you get <strong>fully custom</strong>, <strong>hand-painted</strong> Gel-X press-on nails\",\n",
    "        \"constraints\": [\"NaiLit → NaiLit\", \"press-on nails → ネイルチップ\"],\n",
    "        \"tgt_fr\": \"Avec <strong>NaiLit</strong>, vous obtenez des ongles Gel-X <strong>entièrement personnalisés</strong> et <strong>peints à la main</strong>\",\n",
    "        \"tgt_ja\": \"<strong>NaiLit</strong> なら、<strong>フルカスタム</strong>かつ<strong>手描き</strong>のGel-X ネイルチップが手に入ります\",\n",
    "        \"tgt_it\": \"Con <strong>NaiLit</strong> ottieni unghie Gel-X <strong>completamente personalizzate</strong> e <strong>dipinte a mano</strong>\",\n",
    "    },\n",
    "    {\n",
    "        \"src\": \"<strong>How do I place an order?</strong>\",\n",
    "        \"constraints\": [],\n",
    "        \"tgt_fr\": \"<strong>Comment passer une commande ?</strong>\",\n",
    "        \"tgt_ja\": \"<strong>注文方法を教えてください。</strong>\",\n",
    "        \"tgt_it\": \"<strong>Come posso effettuare un ordine?</strong>\",\n",
    "    },\n",
    "]\n",
    "\n",
    "def render_fewshots(target_lang: str) -> str:\n",
    "    lang_key = {\"fr\": \"tgt_fr\", \"ja\": \"tgt_ja\", \"it\": \"tgt_it\"}[target_lang]\n",
    "    blocks = []\n",
    "    for ex in FEWSHOTS:\n",
    "        cons = ex.get(\"constraints\") or []\n",
    "        cons_txt = \"\"\n",
    "        if cons:\n",
    "            cons_txt = \"Glossary constraints:\\n- \" + \"\\n- \".join(cons) + \"\\n\"\n",
    "        blocks.append(\n",
    "            f\"\"\"### Example\n",
    "Source:\n",
    "{ex['src']}\n",
    "{cons_txt}Target ({target_lang}):\n",
    "{ex[lang_key]}\n",
    "\"\"\"\n",
    "        )\n",
    "    return \"\\n\".join(blocks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Usage Meter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _backoff_sleep(attempt: int, base: float = 0.5, jitter: float = 0.2):\n",
    "    import time, random\n",
    "    time.sleep(base * (2 ** attempt) + random.random() * jitter)\n",
    "\n",
    "# Cache: (lang, src_text, sorted_constraints)\n",
    "CACHE_TRANSLATIONS: Dict[Tuple[str, str, Tuple[str, ...]], str] = {}\n",
    "\n",
    "def safe_usage_tokens(resp):\n",
    "    \"\"\"Return (input_tokens, output_tokens) from OpenAI responses; fallback to (0,0).\"\"\"\n",
    "    try:\n",
    "        u = getattr(resp, \"usage\", None)\n",
    "        if u:\n",
    "            if hasattr(u, \"prompt_tokens\") and hasattr(u, \"completion_tokens\"):\n",
    "                return int(u.prompt_tokens or 0), int(u.completion_tokens or 0)\n",
    "            if hasattr(u, \"input_tokens\") and hasattr(u, \"output_tokens\"):\n",
    "                return int(u.input_tokens or 0), int(u.output_tokens or 0)\n",
    "    except Exception:\n",
    "        pass\n",
    "    return 0, 0\n",
    "\n",
    "import threading, time\n",
    "\n",
    "class UsageMeter:\n",
    "    def __init__(self):\n",
    "        self.lock = threading.Lock()\n",
    "        self.input_tokens = 0\n",
    "        self.output_tokens = 0\n",
    "        self.wall_seconds = 0.0\n",
    "    def add(self, in_toks: int, out_toks: int, dt: float):\n",
    "        with self.lock:\n",
    "            self.input_tokens += int(in_toks or 0)\n",
    "            self.output_tokens += int(out_toks or 0)\n",
    "            self.wall_seconds += float(dt or 0.0)\n",
    "    def snapshot(self):\n",
    "        with self.lock:\n",
    "            return {\n",
    "                \"input_tokens\": self.input_tokens,\n",
    "                \"output_tokens\": self.output_tokens,\n",
    "                \"wall_seconds\": self.wall_seconds,\n",
    "            }\n",
    "\n",
    "CURRENT_METER: Optional[UsageMeter] = None\n",
    "\n",
    "def openai_chat_with_usage(model: str, system_prompt: str, user_prompt: str) -> str:\n",
    "    global CURRENT_METER\n",
    "    t0 = time.time()\n",
    "    resp = OPENAI_CLIENT.chat.completions.create(\n",
    "        model=model,\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": system_prompt},\n",
    "            {\"role\": \"user\",   \"content\": user_prompt},\n",
    "        ],\n",
    "        temperature=0.2,\n",
    "        max_tokens=2048,\n",
    "    )\n",
    "    dt = time.time() - t0\n",
    "    in_toks, out_toks = safe_usage_tokens(resp)\n",
    "    if CURRENT_METER is not None:\n",
    "        CURRENT_METER.add(in_toks, out_toks, dt)\n",
    "    return (resp.choices[0].message.content or \"\").strip()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Translation Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate_segment_with_rag(src_text: str, lang: str, precomputed_constraints: Optional[List[str]] = None) -> str:\n",
    "    # 0) TM exact match\n",
    "    tm_hit = tm_lookup(src_text, lang)\n",
    "    if tm_hit:\n",
    "        return tm_hit\n",
    "\n",
    "    # 1) Build constraints + cache key\n",
    "    constraints = precomputed_constraints if precomputed_constraints is not None else build_constraints(src_text, lang, top_k=3)\n",
    "    constraints_sorted = tuple(sorted(constraints)) if constraints else tuple()\n",
    "    cache_key = (lang, src_text, constraints_sorted)\n",
    "    if cache_key in CACHE_TRANSLATIONS:\n",
    "        return CACHE_TRANSLATIONS[cache_key]\n",
    "\n",
    "    # 2) Few-shots + constraint text\n",
    "    fewshots = render_fewshots(lang)\n",
    "    constraint_text = \"\"\n",
    "    if constraints_sorted:\n",
    "        constraint_text = \"Use these glossary mappings exactly when relevant:\\n- \" + \"\\n- \".join(constraints_sorted) + \"\\n\"\n",
    "\n",
    "    # 3) Prompts\n",
    "    system_prompt = (\n",
    "        \"You are a precise, format-strict translator. \"\n",
    "        \"Reply with only the target text enclosed in <translation>...</translation>.\"\n",
    "    )\n",
    "    user_prompt = f\"\"\"\n",
    "You are a professional localization translator. Translate the source into {lang}.\n",
    "Requirements:\n",
    "- Preserve all HTML tags exactly (do not add/remove/reorder tags).\n",
    "- Keep brand names and DNT terms as-is (case-sensitive), e.g., NaiLit.\n",
    "- Be natural, fluent, and consistent with terminology.\n",
    "- Follow the glossary mappings when relevant (do not hallucinate).\n",
    "- Return ONLY the translation between <translation> and </translation>. Do not add notes.\n",
    "\n",
    "{fewshots}\n",
    "\n",
    "{constraint_text}Source:\n",
    "{src_text}\n",
    "\n",
    "<translation>\n",
    "\"\"\".strip()\n",
    "\n",
    "    model_name = (\n",
    "        (OPENAI_MODELS.get(\"gpt-4o-mini\", {}).get(\"model\") if \"OPENAI_MODELS\" in globals() else None)\n",
    "        or (globals().get(\"RAG_MODEL\", {}).get(\"config\", {}).get(\"model\"))\n",
    "        or os.getenv(\"OPENAI_RAG_MODEL\", \"gpt-4o-mini\")\n",
    "    )\n",
    "\n",
    "    # 4) Call OpenAI with retries\n",
    "    def _call_openai(prompt: str) -> str:\n",
    "        return openai_chat_with_usage(model_name, system_prompt, prompt)\n",
    "\n",
    "    raw = None\n",
    "    for attempt in range(3):\n",
    "        try:\n",
    "            raw = _call_openai(user_prompt)\n",
    "            break\n",
    "        except Exception as e:\n",
    "            if attempt == 2:\n",
    "                print(f\"⚠️ RAG call failed (final): {e}\")\n",
    "                CACHE_TRANSLATIONS[cache_key] = \"[RAG_TRANSLATION_ERROR]\"\n",
    "                return CACHE_TRANSLATIONS[cache_key]\n",
    "            _backoff_sleep(attempt)\n",
    "\n",
    "    # 5) Extract inside <translation>…</translation>\n",
    "    m = re.search(r\"<translation>([\\s\\S]*?)</translation>\", raw or \"\")\n",
    "    translation = (m.group(1).strip() if m else (raw or \"\").strip())\n",
    "\n",
    "    # 6) Validate tags; one corrective retry if needed\n",
    "    if not tags_preserved(src_text, translation):\n",
    "        retry_prompt = user_prompt.replace(\n",
    "            \"<translation>\",\n",
    "            \"IMPORTANT: Copy every HTML tag exactly as in the source. Only the translation.\\n\\n<translation>\"\n",
    "        )\n",
    "        try:\n",
    "            raw2 = _call_openai(retry_prompt)\n",
    "            m2 = re.search(r\"<translation>([\\s\\S]*?)</translation>\", raw2 or \"\")\n",
    "            translation2 = (m2.group(1).strip() if m2 else (raw2 or \"\").strip())\n",
    "            if tags_preserved(src_text, translation2):\n",
    "                translation = translation2\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "    CACHE_TRANSLATIONS[cache_key] = translation\n",
    "    return translation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. End-to-End RAG Batch T9N Pipeline Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pipeline configuration:\n",
      " Languages: fr, ja, it\n",
      " Segments: 76\n",
      " Max workers: 4\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "import os, json, time\n",
    "from pathlib import Path\n",
    "from typing import Dict, List, Tuple, Any\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Config\n",
    "TARGET_LANGUAGES = [\"fr\", \"ja\", \"it\"]\n",
    "LANGUAGE_NAMES = {\"fr\": \"French\", \"ja\": \"Japanese\", \"it\": \"Italian\"}\n",
    "MAX_WORKERS = max(1, int(os.getenv(\"RAG_MAX_WORKERS\", \"4\")))  # 1 = sequential (safer for rate limits)\n",
    "MODEL_NAME = (globals().get(\"RAG_MODEL\", {}).get(\"name\") or \"gpt-4o-mini\")\n",
    "\n",
    "print(\"Pipeline configuration:\")\n",
    "print(f\" Languages: {', '.join(TARGET_LANGUAGES)}\")\n",
    "print(f\" Segments: {len(en_segments)}\")\n",
    "print(f\" Max workers: {MAX_WORKERS}\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "def translate_all_segments_rag(target_lang: str) -> List[Dict[str, Any]]:\n",
    "    start = time.time()\n",
    "\n",
    "    # --- Deduplicate by source text\n",
    "    src_to_paths: Dict[str, List[str]] = {}\n",
    "    for path, src in en_segments:\n",
    "        src_to_paths.setdefault(src, []).append(path)\n",
    "    unique_srcs = list(src_to_paths.keys())\n",
    "    uniq_count = len(unique_srcs)\n",
    "\n",
    "    # Precompute constraints and TM flags once\n",
    "    src_to_constraints: Dict[str, List[str]] = {}\n",
    "    src_tm_hit: Dict[str, bool] = {}\n",
    "    total_constraints_used = 0\n",
    "    for src in unique_srcs:\n",
    "        cons = build_constraints(src, target_lang, top_k=3)\n",
    "        src_to_constraints[src] = cons\n",
    "        total_constraints_used += len(cons)\n",
    "        src_tm_hit[src] = bool(tm_lookup(src, target_lang))\n",
    "\n",
    "    results_map: Dict[str, str] = {}\n",
    "    tm_hits = sum(1 for v in src_tm_hit.values() if v)\n",
    "\n",
    "    if MAX_WORKERS == 1:\n",
    "        for src in tqdm(unique_srcs, total=uniq_count, desc=f\"RAG → {target_lang} (seq)\"):\n",
    "            try:\n",
    "                results_map[src] = translate_segment_with_rag(src, target_lang, src_to_constraints[src])\n",
    "            except Exception as e:\n",
    "                snippet = (src[:60] + \"…\") if len(src) > 60 else src\n",
    "                print(f\"⚠️ Translate failed: {e} | src≈ {snippet!r}\")\n",
    "                results_map[src] = \"[RAG_TRANSLATION_ERROR]\"\n",
    "    else:\n",
    "        with ThreadPoolExecutor(max_workers=MAX_WORKERS) as ex:\n",
    "            futures = {\n",
    "                ex.submit(translate_segment_with_rag, src, target_lang, src_to_constraints[src]): src\n",
    "                for src in unique_srcs\n",
    "            }\n",
    "            for fut in tqdm(as_completed(futures), total=len(futures), desc=f\"RAG → {target_lang}\"):\n",
    "                src = futures[fut]\n",
    "                try:\n",
    "                    results_map[src] = fut.result()\n",
    "                except Exception as e:\n",
    "                    snippet = (src[:60] + \"…\") if len(src) > 60 else src\n",
    "                    print(f\"⚠️ Translate failed: {e} | src≈ {snippet!r}\")\n",
    "                    results_map[src] = \"[RAG_TRANSLATION_ERROR]\"\n",
    "\n",
    "    # --- Expand back to all segments\n",
    "    out: List[Dict[str, Any]] = []\n",
    "    now = time.strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "    for path, src in en_segments:\n",
    "        constraints = src_to_constraints.get(src, [])\n",
    "        out.append({\n",
    "            \"path\": path,\n",
    "            \"source\": src,\n",
    "            \"translation\": results_map.get(src, \"[RAG_TRANSLATION_ERROR]\"),\n",
    "            \"model\": MODEL_NAME,\n",
    "            \"approach\": \"RAG\",\n",
    "            \"target_lang\": target_lang,\n",
    "            \"tm_hit\": src_tm_hit.get(src, False),\n",
    "            \"constraints_found\": len(constraints),\n",
    "            \"constraints_list\": constraints,\n",
    "            \"timestamp\": now,\n",
    "            **({\"source_sha\": SOURCE_SHA} if SOURCE_SHA else {}),\n",
    "        })\n",
    "\n",
    "    dur = time.time() - start\n",
    "\n",
    "    # --- Save\n",
    "    out_dir = Path(\"translations/rag\")\n",
    "    out_dir.mkdir(parents=True, exist_ok=True)\n",
    "    out_file = out_dir / f\"{target_lang}.json\"\n",
    "    with open(out_file, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(out, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "    seg_per_sec = len(out) / max(dur, 1e-6)\n",
    "    print(f\"✅ RAG translation completed: {target_lang}\")\n",
    "    print(f\"   • Unique sources: {uniq_count}  | Segments: {len(out)}\")\n",
    "    print(f\"   • Duration: {dur:.1f}s  | Speed: {seg_per_sec:.2f} seg/s\")\n",
    "    print(f\"   • TM hits (unique-src): {tm_hits}/{uniq_count} ({tm_hits/max(uniq_count,1):.1%})\")\n",
    "    print(f\"   • Constraints found (unique-src): {total_constraints_used}\")\n",
    "    print(f\"   • Saved: {out_file}\")\n",
    "\n",
    "    return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Execute RAG-enhanced T9N for All Languages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Starting French (fr)…\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "RAG → fr: 100%|████████████████████████████████████████████████████████████████████████| 73/73 [00:15<00:00,  4.63it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ RAG translation completed: fr\n",
      "   • Unique sources: 73  | Segments: 76\n",
      "   • Duration: 24.9s  | Speed: 3.05 seg/s\n",
      "   • TM hits (unique-src): 2/73 (2.7%)\n",
      "   • Constraints found (unique-src): 203\n",
      "   • Saved: translations\\rag\\fr.json\n",
      "\n",
      "Starting Japanese (ja)…\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "RAG → ja: 100%|████████████████████████████████████████████████████████████████████████| 73/73 [00:16<00:00,  4.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ RAG translation completed: ja\n",
      "   • Unique sources: 73  | Segments: 76\n",
      "   • Duration: 38.3s  | Speed: 1.98 seg/s\n",
      "   • TM hits (unique-src): 2/73 (2.7%)\n",
      "   • Constraints found (unique-src): 203\n",
      "   • Saved: translations\\rag\\ja.json\n",
      "\n",
      "Starting Italian (it)…\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "RAG → it: 100%|████████████████████████████████████████████████████████████████████████| 73/73 [00:16<00:00,  4.38it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ RAG translation completed: it\n",
      "   • Unique sources: 73  | Segments: 76\n",
      "   • Duration: 38.2s  | Speed: 1.99 seg/s\n",
      "   • TM hits (unique-src): 2/73 (2.7%)\n",
      "   • Constraints found (unique-src): 203\n",
      "   • Saved: translations\\rag\\it.json\n",
      "\n",
      "======================================================================\n",
      "RAG TRANSLATIONS COMPLETE\n",
      "======================================================================\n",
      "✅ French (fr) — saved: translations\\rag\\fr.json\n",
      "✅ Japanese (ja) — saved: translations\\rag\\ja.json\n",
      "✅ Italian (it) — saved: translations\\rag\\it.json\n",
      "\n",
      "Run summary saved to: eval\\rag_run_summary.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "RAG_RESULTS: Dict[str, List[Dict[str, Any]]] = {}\n",
    "RAG_SUMMARY: Dict[str, Any] = {}\n",
    "RAG_METERS: Dict[str, Dict[str, float]] = {}\n",
    "summary_rows: List[Dict[str, Any]] = []\n",
    "\n",
    "for lang in TARGET_LANGUAGES:\n",
    "    # Optional: keep this if you want a small progress cue\n",
    "    print(f\"\\nStarting {LANGUAGE_NAMES[lang]} ({lang})…\")\n",
    "    CURRENT_METER = UsageMeter()  # fresh meter for this language\n",
    "\n",
    "    t0 = time.time()\n",
    "    try:\n",
    "        res = translate_all_segments_rag(lang)   # writes translations/rag/{lang}.json\n",
    "        dt = time.time() - t0\n",
    "\n",
    "        usage = CURRENT_METER.snapshot()\n",
    "        RAG_METERS[lang] = {**usage}  # tokens + wall time only\n",
    "\n",
    "        RAG_RESULTS[lang] = res\n",
    "\n",
    "        total_segments = len(res)\n",
    "        unique_sources = len({r[\"source\"] for r in res})\n",
    "        tm_hits = sum(1 for r in res if r.get(\"tm_hit\"))\n",
    "        errors = sum(1 for r in res if \"[RAG_TRANSLATION_ERROR]\" in (r.get(\"translation\") or \"\"))\n",
    "        total_constraints = sum(r.get(\"constraints_found\", 0) for r in res)\n",
    "        out_file = Path(\"translations/rag\") / f\"{lang}.json\"\n",
    "\n",
    "        RAG_SUMMARY[lang] = {\n",
    "            \"language\": lang,\n",
    "            \"total_segments\": total_segments,\n",
    "            \"unique_sources\": unique_sources,\n",
    "            \"tm_hits_segments\": tm_hits,\n",
    "            \"tm_hit_rate_segments\": (tm_hits / total_segments) if total_segments else 0.0,\n",
    "            \"errors\": errors,\n",
    "            \"error_rate\": (errors / total_segments) if total_segments else 0.0,\n",
    "            \"total_constraints\": total_constraints,\n",
    "            \"avg_constraints_per_segment\": (total_constraints / total_segments) if total_segments else 0.0,\n",
    "            \"duration_sec\": round(dt, 1),\n",
    "            \"segments_per_sec\": (total_segments / dt) if dt > 0 else 0.0,\n",
    "            \"output_file\": str(out_file),\n",
    "            \"input_tokens\": usage[\"input_tokens\"],\n",
    "            \"output_tokens\": usage[\"output_tokens\"],\n",
    "            \"wall_seconds_metered\": usage[\"wall_seconds\"],\n",
    "        }\n",
    "        summary_rows.append(RAG_SUMMARY[lang])\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Failed {LANGUAGE_NAMES[lang]}: {e}\")\n",
    "        RAG_RESULTS[lang] = []\n",
    "        RAG_SUMMARY[lang] = {\"error\": str(e), \"language\": lang}\n",
    "        summary_rows.append(RAG_SUMMARY[lang])\n",
    "\n",
    "# Optional concise recap (no per-metric details)\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"RAG TRANSLATIONS COMPLETE\")\n",
    "print(\"=\" * 70)\n",
    "for lang in TARGET_LANGUAGES:\n",
    "    s = RAG_SUMMARY.get(lang, {})\n",
    "    if \"error\" in s:\n",
    "        print(f\"❌ {LANGUAGE_NAMES[lang]} ({lang}): {s['error']}\")\n",
    "    else:\n",
    "        print(f\"✅ {LANGUAGE_NAMES[lang]} ({lang}) — saved: {s['output_file']}\")\n",
    "\n",
    "# Save a run summary for later analysis\n",
    "Path(\"eval\").mkdir(parents=True, exist_ok=True)\n",
    "summary_path = Path(\"eval/rag_run_summary.csv\")\n",
    "pd.DataFrame(summary_rows).to_csv(summary_path, index=False)\n",
    "print(f\"\\nRun summary saved to: {summary_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Baseline Loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from typing import Dict, List, Optional, Tuple\n",
    "import os, json\n",
    "\n",
    "def _choose_baseline_dir(base_dir: Path, langs: List[str], prefer: Tuple[str, ...] = ()) -> Optional[Path]:\n",
    "    if not base_dir.exists():\n",
    "        return None\n",
    "    candidates = []\n",
    "    for d in base_dir.iterdir():\n",
    "        if not d.is_dir():\n",
    "            continue\n",
    "        if all((d / f\"{lang}.json\").exists() for lang in langs):\n",
    "            pref_score = 0\n",
    "            dn = d.name.lower()\n",
    "            for i, tok in enumerate(prefer):\n",
    "                if tok and tok.lower() in dn:\n",
    "                    pref_score = max(pref_score, len(prefer) - i)\n",
    "            candidates.append((pref_score, d.stat().st_mtime, d))\n",
    "    if not candidates:\n",
    "        return None\n",
    "    candidates.sort(key=lambda t: (t[0], t[1]), reverse=True)\n",
    "    return candidates[0][2]\n",
    "\n",
    "def load_baseline_results(target_languages: Optional[List[str]] = None) -> Dict[str, List[Dict]]:\n",
    "    langs = target_languages or TARGET_LANGUAGES  # falls back to global\n",
    "    base_dir = Path(\"translations/baseline\")\n",
    "    prefer_tokens = tuple(os.getenv(\"BASELINE_MODEL_HINT\", \"gpt openai claude gemini\").split())\n",
    "\n",
    "    chosen = _choose_baseline_dir(base_dir, langs, prefer=prefer_tokens)\n",
    "    if not chosen:\n",
    "        print(\"⚠️ No baseline results directory with all target languages found in translations/baseline\")\n",
    "        return {}\n",
    "\n",
    "    print(f\"Loading baseline from: {chosen}\")\n",
    "    out: Dict[str, List[Dict]] = {}\n",
    "    for lang in langs:\n",
    "        f = chosen / f\"{lang}.json\"\n",
    "        try:\n",
    "            with open(f, \"r\", encoding=\"utf-8\") as fh:\n",
    "                out[lang] = json.load(fh)\n",
    "            print(f\"  • {lang}: {len(out[lang])} segments\")\n",
    "        except Exception as e:\n",
    "            print(f\"  ❌ Failed to load {lang}: {e}\")\n",
    "            out[lang] = []\n",
    "    return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Quality Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "COMPREHENSIVE RAG EVALUATION (with baseline latency/speed compare)\n",
      "========================================================================\n",
      "Loading baseline from: translations\\baseline\\gpt-4o-mini\n",
      "  • fr: 76 segments\n",
      "  • ja: 76 segments\n",
      "  • it: 76 segments\n",
      "Using baseline eval metrics from: eval\\baseline\\claude-3-5-sonnet\n",
      "\n",
      "French (FR)\n",
      "----------------------------------------\n",
      "  • DNT: 100.0% | Glossary: 97.4% | Tags: 98.7%\n",
      "  • Retrieval precision: 18.4%\n",
      "  • Semantic similarity: 0.972\n",
      "  • Latency & Speed\n",
      "     - RAG:       24.90s  | speed 3.05 seg/s\n",
      "     - Baseline:  263.57s | speed 0.29 seg/s\n",
      "\n",
      "Japanese (JA)\n",
      "----------------------------------------\n",
      "  • DNT: 100.0% | Glossary: 99.3% | Tags: 96.1%\n",
      "  • Retrieval precision: 18.4%\n",
      "  • Semantic similarity: 0.967\n",
      "  • Latency & Speed\n",
      "     - RAG:       38.30s  | speed 1.98 seg/s\n",
      "     - Baseline:  181.96s | speed 0.42 seg/s\n",
      "\n",
      "Italian (IT)\n",
      "----------------------------------------\n",
      "  • DNT: 100.0% | Glossary: 94.3% | Tags: 97.4%\n",
      "  • Retrieval precision: 18.4%\n",
      "  • Semantic similarity: 0.976\n",
      "  • Latency & Speed\n",
      "     - RAG:       38.30s  | speed 1.99 seg/s\n",
      "     - Baseline:  188.27s | speed 0.40 seg/s\n",
      "\n",
      "Saved RAG quality table: eval\\rag_comprehensive_evaluation.csv\n",
      "Saved RAG vs Baseline latency/speed table: eval\\rag_quality_vs_baseline.csv\n",
      "\n",
      "OVERALL RAG PERFORMANCE SUMMARY\n",
      "==================================================\n",
      "Languages evaluated: 3\n",
      "Avg DNT: 100.0%\n",
      "Avg Glossary: 97.0%\n",
      "Avg Tags: 97.4%\n",
      "Avg Retrieval precision: 18.4%\n"
     ]
    }
   ],
   "source": [
    "def comprehensive_rag_evaluation():\n",
    "    print(\"\\nCOMPREHENSIVE RAG EVALUATION (with baseline latency/speed compare)\")\n",
    "    print(\"=\" * 72)\n",
    "\n",
    "    # Handle both return styles: dict OR (dict, chosen_path)\n",
    "    _loaded = load_baseline_results()\n",
    "    if isinstance(_loaded, tuple):\n",
    "        baseline_results, _baseline_dir = _loaded\n",
    "    else:\n",
    "        baseline_results, _baseline_dir = _loaded, None\n",
    "\n",
    "    baseline_eval    = _load_baseline_eval_metrics()  # for baseline latency/speed\n",
    "    results_rows = []\n",
    "    compare_rows  = []\n",
    "\n",
    "    rag_model_name = (\n",
    "        (OPENAI_MODELS.get(\"gpt-4o-mini\", {}).get(\"model\") if \"OPENAI_MODELS\" in globals() else None)\n",
    "        or (globals().get(\"MODEL_NAME\"))\n",
    "        or os.getenv(\"OPENAI_RAG_MODEL\", \"gpt-4o-mini\")\n",
    "    )\n",
    "\n",
    "    for lang in TARGET_LANGUAGES:\n",
    "        if lang not in RAG_RESULTS:\n",
    "            print(f\"⚠️ No RAG results for {lang}\")\n",
    "            continue\n",
    "\n",
    "        rag  = RAG_RESULTS[lang] or []\n",
    "        base = (baseline_results or {}).get(lang, [])\n",
    "        base_lookup = {x.get(\"path\"): x.get(\"translation\", \"\") for x in base if \"path\" in x}\n",
    "\n",
    "        seg_rows = []\n",
    "        rag_hyps, base_refs = [], []\n",
    "\n",
    "        for item in rag:\n",
    "            src = item.get(\"source\", \"\")\n",
    "            tgt = item.get(\"translation\", \"\")\n",
    "            path = item.get(\"path\", \"\")\n",
    "            retrieved = item.get(\"constraints_list\", [])\n",
    "\n",
    "            seg_rows.append({\n",
    "                \"path\": path,\n",
    "                \"dnt_preserved\": dnt_preserved(src, tgt),\n",
    "                \"glossary_adherence\": glossary_adherence(src, tgt, lang),\n",
    "                \"tag_preserved\": tags_preserved(src, tgt),\n",
    "                \"retrieval_precision\": retrieval_precision([t.split(\" → \")[0] for t in retrieved], src),\n",
    "                \"tm_hit\": item.get(\"tm_hit\", False),\n",
    "                \"constraints_count\": len(retrieved),\n",
    "                \"has_error\": \"[RAG_TRANSLATION_ERROR]\" in (tgt or \"\"),\n",
    "            })\n",
    "\n",
    "            if path in base_lookup:\n",
    "                rag_hyps.append(tgt or \"\")\n",
    "                base_refs.append(base_lookup[path] or \"\")\n",
    "\n",
    "        tot = len(seg_rows)\n",
    "        if tot == 0:\n",
    "            print(f\"⚠️ No evaluable segments for {lang}\")\n",
    "            continue\n",
    "\n",
    "        dnt_rate   = sum(r[\"dnt_preserved\"] for r in seg_rows) / tot\n",
    "        gloss_avg  = sum(r[\"glossary_adherence\"] for r in seg_rows) / tot\n",
    "        tag_rate   = sum(r[\"tag_preserved\"] for r in seg_rows) / tot\n",
    "        ret_prec   = sum(r[\"retrieval_precision\"] for r in seg_rows) / tot\n",
    "        tm_hits    = sum(r[\"tm_hit\"] for r in seg_rows)\n",
    "        avg_constr = sum(r[\"constraints_count\"] for r in seg_rows) / tot\n",
    "        err_rate   = sum(r[\"has_error\"] for r in seg_rows) / tot\n",
    "\n",
    "        # Semantic similarity to baseline (cosine via embedding)\n",
    "        sem_sim = None\n",
    "        if rag_hyps and base_refs:\n",
    "            try:\n",
    "                h_emb = EMB_MODEL.encode(rag_hyps, batch_size=64, normalize_embeddings=True)\n",
    "                r_emb = EMB_MODEL.encode(base_refs, batch_size=64, normalize_embeddings=True)\n",
    "                sims = (h_emb * r_emb).sum(axis=1).astype(float)\n",
    "                sims = np.clip(sims, -1.0, 1.0)\n",
    "                sem_sim = float(np.mean(sims))\n",
    "            except Exception as e:\n",
    "                print(f\"  ⚠️ Semantic similarity failed for {lang}: {e}\")\n",
    "                sem_sim = None\n",
    "\n",
    "        # RAG latency/speed from RAG_SUMMARY (measured)\n",
    "        rag_lat   = (RAG_SUMMARY.get(lang) or {}).get(\"duration_sec\")\n",
    "        rag_speed = (RAG_SUMMARY.get(lang) or {}).get(\"segments_per_sec\")\n",
    "\n",
    "        # Baseline latency/speed from saved metrics (if present)\n",
    "        base_lat   = None\n",
    "        base_speed = None\n",
    "        if lang in baseline_eval:\n",
    "            base_lat   = baseline_eval[lang].get(\"total_duration_sec\")\n",
    "            base_speed = baseline_eval[lang].get(\"segments_per_minute\")\n",
    "            if isinstance(base_speed, (int, float)) and base_speed:\n",
    "                base_speed = round(base_speed / 60.0, 4)  # seg/sec\n",
    "\n",
    "        row = {\n",
    "            \"language\": lang,\n",
    "            \"total_segments\": tot,\n",
    "            \"dnt_preservation_rate\": round(dnt_rate, 3),\n",
    "            \"glossary_adherence_avg\": round(gloss_avg, 3),\n",
    "            \"tag_preservation_rate\": round(tag_rate, 3),\n",
    "            \"retrieval_precision_avg\": round(ret_prec, 3),\n",
    "            \"semantic_similarity_avg\": round(sem_sim, 3) if sem_sim is not None else None,\n",
    "            \"tm_hits_found\": int(tm_hits),\n",
    "            \"tm_entries_available\": len(TM_DICT.get(lang, {})),\n",
    "            \"avg_constraints_per_segment\": round(avg_constr, 2),\n",
    "            \"error_rate\": round(err_rate, 3),\n",
    "            \"evaluation_timestamp\": time.strftime(\"%Y-%m-%d %H:%M:%S\"),\n",
    "        }\n",
    "        results_rows.append(row)\n",
    "\n",
    "        compare_rows.append({\n",
    "            \"language\": lang,\n",
    "            \"rag_model\": rag_model_name,\n",
    "            \"rag_duration_sec\": rag_lat,\n",
    "            \"rag_segments_per_sec\": rag_speed,\n",
    "            \"baseline_duration_sec\": base_lat,\n",
    "            \"baseline_segments_per_sec\": base_speed,\n",
    "            # headline quality signal\n",
    "            \"rag_tag_preservation\": row[\"tag_preservation_rate\"],\n",
    "        })\n",
    "\n",
    "        def _fmt(x, unit=\"\"):\n",
    "            return (\"n/a\" if x is None else f\"{x:.2f}{unit}\")\n",
    "\n",
    "        print(f\"\\n{LANGUAGE_NAMES[lang]} ({lang.upper()})\")\n",
    "        print(\"-\" * 40)\n",
    "        print(f\"  • DNT: {row['dnt_preservation_rate']:.1%} | Glossary: {row['glossary_adherence_avg']:.1%} | Tags: {row['tag_preservation_rate']:.1%}\")\n",
    "        print(f\"  • Retrieval precision: {row['retrieval_precision_avg']:.1%}\")\n",
    "        if row[\"semantic_similarity_avg\"] is not None:\n",
    "            print(f\"  • Semantic similarity: {row['semantic_similarity_avg']:.3f}\")\n",
    "        print(\"  • Latency & Speed\")\n",
    "        print(f\"     - RAG:       {_fmt(rag_lat,'s')}  | speed {_fmt(rag_speed,' seg/s')}\")\n",
    "        print(f\"     - Baseline:  {_fmt(base_lat,'s')} | speed {_fmt(base_speed,' seg/s')}\")\n",
    "\n",
    "    # Save aggregated quality table\n",
    "    if results_rows:\n",
    "        rag_df = pd.DataFrame(results_rows)\n",
    "        out1 = Path(\"eval/rag_comprehensive_evaluation.csv\")\n",
    "        out1.parent.mkdir(parents=True, exist_ok=True)\n",
    "        rag_df.to_csv(out1, index=False)\n",
    "        print(f\"\\nSaved RAG quality table: {out1}\")\n",
    "    else:\n",
    "        rag_df = None\n",
    "\n",
    "    # Save latency/speed compare\n",
    "    if compare_rows:\n",
    "        compare_df = pd.DataFrame(compare_rows)\n",
    "        out2 = Path(\"eval/rag_quality_vs_baseline.csv\")\n",
    "        out2.parent.mkdir(parents=True, exist_ok=True)\n",
    "        compare_df.to_csv(out2, index=False)\n",
    "        print(f\"Saved RAG vs Baseline latency/speed table: {out2}\")\n",
    "    else:\n",
    "        compare_df = None\n",
    "\n",
    "    # Overall rollup\n",
    "    if rag_df is not None and not rag_df.empty:\n",
    "        print(\"\\nOVERALL RAG PERFORMANCE SUMMARY\")\n",
    "        print(\"=\" * 50)\n",
    "        print(f\"Languages evaluated: {len(rag_df)}\")\n",
    "        print(f\"Avg DNT: {rag_df['dnt_preservation_rate'].mean():.1%}\")\n",
    "        print(f\"Avg Glossary: {rag_df['glossary_adherence_avg'].mean():.1%}\")\n",
    "        print(f\"Avg Tags: {rag_df['tag_preservation_rate'].mean():.1%}\")\n",
    "        print(f\"Avg Retrieval precision: {rag_df['retrieval_precision_avg'].mean():.1%}\")\n",
    "\n",
    "    return rag_df, compare_df\n",
    "\n",
    "# Run the integrated evaluation\n",
    "RAG_DF, RAG_VS_BASELINE_DF = comprehensive_rag_evaluation()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
