{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RAG-Enhanced Translation System\n",
    "\n",
    "This notebook builds on the baseline results from `Baseline.ipynb`.\n",
    "\n",
    "**Features:**\n",
    "- **Adaptive Retrieval**: Context-aware glossary and TM retrieval\n",
    "- **Comprehensive Evaluation**: RAG vs baseline comparison\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup & Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RAG-Enhanced Translation System (Clean Rewrite)\n",
      "======================================================================\n",
      "Loaded 76 source segments\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import json\n",
    "import time\n",
    "import random\n",
    "import warnings\n",
    "import hashlib\n",
    "import threading\n",
    "from pathlib import Path\n",
    "from typing import Any, Dict, List, Tuple, Optional\n",
    "\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from dotenv import load_dotenv\n",
    "from tqdm import tqdm\n",
    "\n",
    "# RAG-specific\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import chromadb\n",
    "\n",
    "# OpenAI client\n",
    "from openai import OpenAI\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "pd.set_option(\"display.max_columns\", None)\n",
    "pd.set_option(\"display.width\", None)\n",
    "\n",
    "load_dotenv()\n",
    "print(\"RAG-Enhanced Translation System (Clean Rewrite)\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "def flatten_json_strings(obj: Any, prefix: str = \"\") -> List[Tuple[str, str]]:\n",
    "    out: List[Tuple[str, str]] = []\n",
    "    if isinstance(obj, dict):\n",
    "        for k, v in obj.items():\n",
    "            new_prefix = f\"{prefix}.{k}\" if prefix else k\n",
    "            out.extend(flatten_json_strings(v, new_prefix))\n",
    "    elif isinstance(obj, list):\n",
    "        for i, v in enumerate(obj):\n",
    "            new_prefix = f\"{prefix}[{i}]\"\n",
    "            out.extend(flatten_json_strings(v, new_prefix))\n",
    "    elif isinstance(obj, str) and obj.strip():\n",
    "        out.append((prefix, obj))\n",
    "    return out\n",
    "\n",
    "SRC_FILE = Path(\"data/en.json\")\n",
    "if not SRC_FILE.exists():\n",
    "    raise FileNotFoundError(\"Missing data/en.json\")\n",
    "\n",
    "with open(SRC_FILE, \"r\", encoding=\"utf-8\") as f:\n",
    "    en_json = json.load(f)\n",
    "\n",
    "en_segments: List[Tuple[str, str]] = flatten_json_strings(en_json)\n",
    "print(f\"Loaded {len(en_segments)} source segments\")\n",
    "\n",
    "try:\n",
    "    def _sha1(p): \n",
    "        h = hashlib.sha1()\n",
    "        with open(p, 'rb') as f:\n",
    "            for chunk in iter(lambda: f.read(8192), b''): \n",
    "                h.update(chunk)\n",
    "        return h.hexdigest()\n",
    "    SOURCE_SHA = _sha1(\"data/en.json\")\n",
    "except Exception:\n",
    "    SOURCE_SHA = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load Translation Memory (TM) & Glossary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TM FR: 2 entries\n",
      "TM JA: 2 entries\n",
      "TM IT: 2 entries\n",
      "Glossary FR: 18 mappings\n",
      "Glossary JA: 18 mappings\n",
      "Glossary IT: 18 mappings\n",
      "DNT terms: ['Gel-X', 'NaiLit']\n"
     ]
    }
   ],
   "source": [
    "def load_translation_memory() -> Dict[str, Dict[str, str]]:\n",
    "    tm_dict = {\"fr\": {}, \"ja\": {}, \"it\": {}}\n",
    "    tm_file = Path(\"data/translation_memory.csv\")\n",
    "    if tm_file.exists():\n",
    "        try:\n",
    "            df = pd.read_csv(tm_file)\n",
    "            for _, row in df.iterrows():\n",
    "                lang = row.get(\"tgt_lang\")\n",
    "                src = row.get(\"src_text\")\n",
    "                tgt = row.get(\"tgt_text\")\n",
    "                if lang in tm_dict and isinstance(src, str) and isinstance(tgt, str) and src and tgt:\n",
    "                    tm_dict[lang][src] = tgt\n",
    "            for lang in [\"fr\", \"ja\", \"it\"]:\n",
    "                print(f\"TM {lang.upper()}: {len(tm_dict[lang])} entries\")\n",
    "        except Exception as e:\n",
    "            print(f\"Could not load TM: {e}\")\n",
    "    else:\n",
    "        print(\"No translation_memory.csv found\")\n",
    "    return tm_dict\n",
    "\n",
    "\n",
    "def load_glossary() -> Tuple[List[str], Dict[str, Dict[str, str]], List[str]]:\n",
    "    glossary_terms: List[str] = []\n",
    "    glossary_map: Dict[str, Dict[str, str]] = {\"fr\": {}, \"ja\": {}, \"it\": {}}\n",
    "    dnt_terms: List[str] = [\"NaiLit\"]\n",
    "\n",
    "    gl_file = Path(\"data/glossary.csv\")\n",
    "    if gl_file.exists():\n",
    "        try:\n",
    "            df = pd.read_csv(gl_file)\n",
    "            if \"source_term\" not in df.columns:\n",
    "                raise ValueError(\"glossary.csv must have a 'source_term' column\")\n",
    "            glossary_terms = [t for t in df[\"source_term\"].dropna().astype(str).tolist() if t]\n",
    "            for lang in [\"fr\", \"ja\", \"it\"]:\n",
    "                if lang in df.columns:\n",
    "                    col = df[lang].astype(str)\n",
    "                    glossary_map[lang] = {\n",
    "                        st: tt for st, tt in zip(df[\"source_term\"], col)\n",
    "                        if pd.notna(st) and pd.notna(tt)\n",
    "                    }\n",
    "                    print(f\"Glossary {lang.upper()}: {len(glossary_map[lang])} mappings\")\n",
    "            if \"dnt\" in df.columns:\n",
    "                mask = df[\"dnt\"].astype(str).str.upper() == \"TRUE\"\n",
    "                dnt_terms.extend(df.loc[mask, \"source_term\"].dropna().astype(str).tolist())\n",
    "        except Exception as e:\n",
    "            print(f\"Could not load glossary: {e}\")\n",
    "    else:\n",
    "        print(\"No glossary.csv found\")\n",
    "\n",
    "    dnt_terms = sorted(list({t for t in dnt_terms if t}))\n",
    "    return glossary_terms, glossary_map, dnt_terms\n",
    "\n",
    "TM_DICT = load_translation_memory()\n",
    "GLOSSARY_TERMS, GLOSSARY_MAP, DNT_TERMS = load_glossary()\n",
    "print(f\"DNT terms: {DNT_TERMS}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Embeddings & Vector DB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading embedding model…\n",
      "Indexing glossary…\n",
      "Glossary ready in Chroma\n"
     ]
    }
   ],
   "source": [
    "print(\"Loading embedding model…\")\n",
    "EMB_MODEL = SentenceTransformer(\"intfloat/multilingual-e5-base\")\n",
    "\n",
    "CHROMA_PATH = \".chroma\"\n",
    "chroma_client = chromadb.PersistentClient(path=CHROMA_PATH)\n",
    "GLOSSARY_COL = chroma_client.get_or_create_collection(\n",
    "    name=\"glossary\", \n",
    "    metadata={\"hnsw:space\": \"cosine\"}\n",
    ")\n",
    "\n",
    "if GLOSSARY_TERMS:\n",
    "    print(\"Indexing glossary…\")\n",
    "    ids_all = [str(i) for i in range(len(GLOSSARY_TERMS))]\n",
    "    try:\n",
    "        existing = set(GLOSSARY_COL.get(ids=ids_all)[\"ids\"])  # may raise if none\n",
    "    except Exception:\n",
    "        existing = set()\n",
    "    to_add = [(i, t) for i, t in enumerate(GLOSSARY_TERMS) if str(i) not in existing]\n",
    "    if to_add:\n",
    "        batch_terms = [t for _, t in to_add]\n",
    "        embs = EMB_MODEL.encode(batch_terms, batch_size=64, normalize_embeddings=True, show_progress_bar=True)\n",
    "        GLOSSARY_COL.add(\n",
    "            ids=[str(i) for i, _ in to_add], \n",
    "            documents=batch_terms, \n",
    "            embeddings=[e.tolist() for e in embs]\n",
    "        )\n",
    "    print(\"Glossary ready in Chroma\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Retrieval & Utility Helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "HTML_TAG = re.compile(r\"</?\\w+(?:\\s+[^>]*?)?>\", re.IGNORECASE)\n",
    "_WORD = re.compile(r\"\\w+\", re.UNICODE)\n",
    "\n",
    "def _normalize_for_retrieval(s: str) -> str:\n",
    "    s = HTML_TAG.sub(\"\", s or \"\")\n",
    "    s = re.sub(r\"\\s+\", \" \", s).strip().lower()\n",
    "    return s\n",
    "\n",
    "def _tokenize(s: str) -> set:\n",
    "    return set(w.lower() for w in _WORD.findall(s or \"\"))\n",
    "\n",
    "def tm_lookup(src_text: str, lang: str) -> Optional[str]:\n",
    "    return TM_DICT.get(lang, {}).get(src_text)\n",
    "\n",
    "def tags_preserved(src: str, tgt: str) -> bool:\n",
    "    return HTML_TAG.findall(src or \"\") == HTML_TAG.findall(tgt or \"\")\n",
    "\n",
    "def retrieve_glossary_terms(\n",
    "    segment_text: str, \n",
    "    top_k: int = 5,\n",
    "    min_score: float = 0.45, \n",
    "    overfetch: int = 24,\n",
    "    require_lex_for_long: bool = True\n",
    ") -> List[str]:\n",
    "    if not (GLOSSARY_COL and GLOSSARY_TERMS):\n",
    "        return []\n",
    "    norm = _normalize_for_retrieval(segment_text)\n",
    "    # E5 query style\n",
    "    q_vec = EMB_MODEL.encode([f\"query: {norm}\"], normalize_embeddings=True)[0].tolist()\n",
    "    try:\n",
    "        res = GLOSSARY_COL.query(query_embeddings=[q_vec], n_results=max(top_k * 3, overfetch))\n",
    "        docs: List[str] = res.get(\"documents\", [[]])[0] if res else []\n",
    "        dists = res.get(\"distances\", [[]])[0] if res else []\n",
    "        sims = [(1.0 - d) if (0.0 <= d <= 2.0) else d for d in dists] if dists else [0.0] * len(docs)\n",
    "    except Exception as e:\n",
    "        print(f\"Retrieval error: {e}\")\n",
    "        return []\n",
    "\n",
    "    seg_tokens = _tokenize(norm)\n",
    "\n",
    "    def _lex_boost(term: str) -> float:\n",
    "        t = term.lower()\n",
    "        b = 0.0\n",
    "        if t in norm:\n",
    "            b += 0.12\n",
    "        overlap = len(seg_tokens & _tokenize(t))\n",
    "        if overlap:\n",
    "            b += min(0.05 * overlap, 0.15)\n",
    "        return b\n",
    "\n",
    "    long_text = len(norm) >= 80 if require_lex_for_long else False\n",
    "    scored: Dict[str, float] = {}\n",
    "    for term, base in zip(docs, sims):\n",
    "        lb = _lex_boost(term)\n",
    "        if long_text and lb == 0.0:\n",
    "            continue\n",
    "        score = base + lb\n",
    "        if score >= min_score:\n",
    "            if term not in scored or score > scored[term]:\n",
    "                scored[term] = score\n",
    "\n",
    "    ranked = sorted(scored.items(), key=lambda x: x[1], reverse=True)\n",
    "    return [t for t, _ in ranked[:top_k]]\n",
    "\n",
    "def build_constraints(src_text: str, lang: str, top_k: int = 3) -> List[str]:\n",
    "    if lang not in GLOSSARY_MAP:\n",
    "        return []\n",
    "    terms = retrieve_glossary_terms(src_text, top_k=top_k, min_score=0.45)\n",
    "    pairs: List[str] = []\n",
    "    for en_term in terms:\n",
    "        tgt = GLOSSARY_MAP[lang].get(en_term)\n",
    "        if isinstance(tgt, str) and tgt:\n",
    "            pairs.append(f\"{en_term} → {tgt}\")\n",
    "    return pairs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Open AI client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OPENAI models: {'gpt-4o-mini': {'model': 'gpt-4o-mini'}}\n"
     ]
    }
   ],
   "source": [
    "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\n",
    "if not OPENAI_API_KEY:\n",
    "    raise RuntimeError(\"Missing OPENAI_API_KEY\")\n",
    "\n",
    "OPENAI_CLIENT = OpenAI(api_key=OPENAI_API_KEY)\n",
    "\n",
    "try:\n",
    "    OPENAI_MODELS\n",
    "except NameError:\n",
    "    OPENAI_MODELS = {}\n",
    "\n",
    "try:\n",
    "    BASELINE_MODELS\n",
    "except NameError:\n",
    "    BASELINE_MODELS = {}\n",
    "\n",
    "OPENAI_MODELS[\"gpt-4o-mini\"] = {\n",
    "    \"model\": os.getenv(\"OPENAI_BASELINE_MODEL\", \"gpt-4o-mini\")\n",
    "}\n",
    "BASELINE_MODELS[\"gpt-4o-mini\"] = (\"openai\", OPENAI_MODELS[\"gpt-4o-mini\"])\n",
    "\n",
    "print(\"OPENAI models:\", OPENAI_MODELS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Few-shots & Prompt helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "FEWSHOT_EXAMPLES = {\n",
    "    \"fr\": [\n",
    "        (\"Each beautiful press-on nail is carefully crafted by <strong>our in-house</strong> nail techs.\",\n",
    "            \"Chaque beau press-on nail est soigneusement conçu par nos stylistes ongulaires <strong>maison</strong>.\"),\n",
    "        (\"Instead of using brittle acrylic nail bases, NaiLit uses Gel-X soft gel nail bases that provide just the right curvature and comfort for your natural nails.\",\n",
    "            \"Au lieu d’utiliser des bases d’ongles fragiles en acrylic, NaiLit utilise des bases en gel souple Gel-X qui offrent la courbure et le confort idéaux pour vos ongles naturels.\"),\n",
    "        (\"Whether you feel like elegant nude French coffin nails or trendy seafoam cat eye stiletto nails, NaiLit has just the right options for you to fully customize.\",\n",
    "            \"Que vous ayez envie d’un nude French ballerine élégant ou d’un vert d’eau cat-eye pointu tendance, NaiLit vous propose les options idéales pour une personnalisation totale.\"),\n",
    "    ],\n",
    "    \"ja\": [\n",
    "        (\"Each beautiful press-on nail is carefully crafted by <strong>our in-house</strong> nail techs.\",\n",
    "            \"それぞれの美しい press-on nail は、<strong>当社専属</strong>のネイリストが丁寧に仕上げています。\"),\n",
    "        (\"Instead of using brittle acrylic nail bases, NaiLit uses Gel-X soft gel nail bases that provide just the right curvature and comfort for your natural nails.\",\n",
    "            \"割れやすいアクリルベースの代わりに、NaiLit は柔らかいジェルの Gel-X ベースを使用し、地爪に最適なカーブと快適さを提供します。\"),\n",
    "        (\"Whether you feel like elegant nude French coffin nails or trendy seafoam cat eye stiletto nails, NaiLit has just the right options for you to fully customize.\",\n",
    "            \"エレガントなヌーディーのバレリーナフレンチや、トレンディなミント系のマグネット（cat-eye）ポイントネイルなど、NaiLit なら思い通りにカスタマイズできます。\"),\n",
    "    ],\n",
    "    \"it\": [\n",
    "        (\"Each beautiful press-on nail is carefully crafted by <strong>our in-house</strong> nail techs.\",\n",
    "            \"Ogni bellissima press-on nail è accuratamente realizzata dalle nostre onicotecniche <strong>della casa</strong>.\"),\n",
    "        (\"Instead of using brittle acrylic nail bases, NaiLit uses Gel-X soft gel nail bases that provide just the right curvature and comfort for your natural nails.\",\n",
    "            \"Invece di utilizzare basi per unghie fragili in acrylic, NaiLit impiega basi in gel morbido Gel-X che garantiscono la curvatura e il comfort perfetti per le tue unghie naturali.\"),\n",
    "        (\"Whether you feel like elegant nude French coffin nails or trendy seafoam cat eye stiletto nails, NaiLit has just the right options for you to fully customize.\",\n",
    "            \"Che tu abbia voglia di un’elegante nude French ballerina o di un trendy verde acqua cat-eye stiletto, NaiLit ha le opzioni perfette per personalizzare al massimo.\"),\n",
    "    ],\n",
    "}\n",
    "\n",
    "LANGUAGE_NAMES_HUMAN = {\"fr\": \"French\", \"ja\": \"Japanese\", \"it\": \"Italian\"}\n",
    "\n",
    "def render_fewshots(target_lang: str, constraints: list[str] | None = None) -> str:\n",
    "    \"\"\"Render few-shot examples and optional glossary constraints for a given language.\"\"\"\n",
    "    pairs = FEWSHOT_EXAMPLES.get(target_lang, [])\n",
    "    examples_txt = []\n",
    "    for i, (src, tgt) in enumerate(pairs, 1):\n",
    "        examples_txt.append(\n",
    "            f\"Example {i}:\\nSource:\\n{src}\\nTarget ({target_lang}):\\n{tgt}\\n\"\n",
    "        )\n",
    "\n",
    "    cons_txt = \"\"\n",
    "    if constraints:\n",
    "        cons_txt = \"Glossary constraints (use exactly when relevant):\\n\" + \"\\n\".join(\n",
    "            f\"- {c}\" for c in constraints\n",
    "        ) + \"\\n\\n\"\n",
    "\n",
    "    return (\n",
    "        cons_txt\n",
    "        + \"### Few-shot Examples\\n\"\n",
    "        + \"\\n\".join(examples_txt)\n",
    "        + \"\\n\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Usage Meter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _backoff_sleep(attempt: int, base: float = 0.5, jitter: float = 0.2):\n",
    "    import time, random\n",
    "    time.sleep(base * (2 ** attempt) + random.random() * jitter)\n",
    "\n",
    "# Cache: (lang, src_text, sorted_constraints)\n",
    "CACHE_TRANSLATIONS: Dict[Tuple[str, str, Tuple[str, ...]], str] = {}\n",
    "\n",
    "def safe_usage_tokens(resp):\n",
    "    \"\"\"Return (input_tokens, output_tokens) from OpenAI responses; fallback to (0,0).\"\"\"\n",
    "    try:\n",
    "        u = getattr(resp, \"usage\", None)\n",
    "        if u:\n",
    "            if hasattr(u, \"prompt_tokens\") and hasattr(u, \"completion_tokens\"):\n",
    "                return int(u.prompt_tokens or 0), int(u.completion_tokens or 0)\n",
    "            if hasattr(u, \"input_tokens\") and hasattr(u, \"output_tokens\"):\n",
    "                return int(u.input_tokens or 0), int(u.output_tokens or 0)\n",
    "    except Exception:\n",
    "        pass\n",
    "    return 0, 0\n",
    "\n",
    "import threading, time\n",
    "\n",
    "class UsageMeter:\n",
    "    def __init__(self):\n",
    "        self.lock = threading.Lock()\n",
    "        self.input_tokens = 0\n",
    "        self.output_tokens = 0\n",
    "        self.wall_seconds = 0.0\n",
    "    def add(self, in_toks: int, out_toks: int, dt: float):\n",
    "        with self.lock:\n",
    "            self.input_tokens += int(in_toks or 0)\n",
    "            self.output_tokens += int(out_toks or 0)\n",
    "            self.wall_seconds += float(dt or 0.0)\n",
    "    def snapshot(self):\n",
    "        with self.lock:\n",
    "            return {\n",
    "                \"input_tokens\": self.input_tokens,\n",
    "                \"output_tokens\": self.output_tokens,\n",
    "                \"wall_seconds\": self.wall_seconds,\n",
    "            }\n",
    "\n",
    "CURRENT_METER: Optional[UsageMeter] = None\n",
    "\n",
    "def openai_chat_with_usage(model: str, system_prompt: str, user_prompt: str) -> str:\n",
    "    global CURRENT_METER\n",
    "    t0 = time.time()\n",
    "    resp = OPENAI_CLIENT.chat.completions.create(\n",
    "        model=model,\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": system_prompt},\n",
    "            {\"role\": \"user\",   \"content\": user_prompt},\n",
    "        ],\n",
    "        temperature=0.2,\n",
    "        max_tokens=2048,\n",
    "    )\n",
    "    dt = time.time() - t0\n",
    "    in_toks, out_toks = safe_usage_tokens(resp)\n",
    "    if CURRENT_METER is not None:\n",
    "        CURRENT_METER.add(in_toks, out_toks, dt)\n",
    "    return (resp.choices[0].message.content or \"\").strip()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Translation Function (Single Segment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_RAG_RETRIES = int(os.getenv(\"RAG_MAX_RETRIES\", \"3\"))\n",
    "\n",
    "def _extract_translation_block(text: str) -> str:\n",
    "    # Return content inside <translation>...</translation>, or raw fallback\n",
    "    m = re.search(r\"<translation>([\\s\\S]*?)</translation>\", text or \"\", re.IGNORECASE)\n",
    "    return (m.group(1).strip() if m else (text or \"\").strip())\n",
    "\n",
    "def _build_user_prompt(src_text: str, lang: str, constraints: list[str] | None = None) -> str:\n",
    "    # Compose the user prompt with shared few-shots and optional constraints\n",
    "    lang_name = (LANGUAGE_NAMES_HUMAN.get(lang, lang) if \"LANGUAGE_NAMES_HUMAN\" in globals() else lang)\n",
    "    fewshots_block = render_fewshots(lang, constraints=constraints)\n",
    "    return (\n",
    "        f\"You are a professional UX translator. Translate the source into {lang_name}.\\n\"\n",
    "        \"Requirements:\\n\"\n",
    "        \"- Preserve all HTML tags exactly (do not add/remove/reorder tags).\\n\"\n",
    "        \"- Keep brand names and DNT terms as-is (case-sensitive), e.g., NaiLit.\\n\"\n",
    "        \"- Be natural, fluent, and consistent with terminology.\\n\"\n",
    "        \"- When glossary constraints are provided, apply them exactly when relevant.\\n\"\n",
    "        \"- Return ONLY the translation between <translation> and </translation>. Do not add notes.\\n\\n\"\n",
    "        f\"{fewshots_block}\"\n",
    "        \"Source:\\n\"\n",
    "        f\"{src_text}\\n\\n\"\n",
    "        \"<translation>\"\n",
    "    )\n",
    "\n",
    "def _model_name_for_rag() -> str:\n",
    "    # Resolve the model string with fallbacks\n",
    "    return (\n",
    "        (OPENAI_MODELS.get(\"gpt-4o-mini\", {}).get(\"model\") if \"OPENAI_MODELS\" in globals() else None)\n",
    "        or (globals().get(\"RAG_MODEL\", {}).get(\"config\", {}).get(\"model\"))\n",
    "        or os.getenv(\"OPENAI_RAG_MODEL\", \"gpt-4o-mini\")\n",
    "    )\n",
    "\n",
    "def translate_segment_with_rag(src_text: str, lang: str, precomputed_constraints: Optional[List[str]] = None) -> str:\n",
    "    # 0) TM exact match\n",
    "    tm_hit = tm_lookup(src_text, lang)\n",
    "    if tm_hit:\n",
    "        return tm_hit\n",
    "\n",
    "    # 1) Constraints + cache key\n",
    "    constraints = (\n",
    "        precomputed_constraints\n",
    "        if precomputed_constraints is not None\n",
    "        else build_constraints(src_text, lang, top_k=3)\n",
    "    )\n",
    "    constraints_sorted = tuple(sorted(constraints)) if constraints else tuple()\n",
    "    cache_key = (lang, src_text, constraints_sorted)\n",
    "    if cache_key in CACHE_TRANSLATIONS:\n",
    "        return CACHE_TRANSLATIONS[cache_key]\n",
    "\n",
    "    # 2) Prompts\n",
    "    system_prompt = (\n",
    "        \"You are a precise, format-strict translator. \"\n",
    "        \"Reply with only the target text enclosed in <translation>...</translation>.\"\n",
    "    )\n",
    "    user_prompt = _build_user_prompt(src_text, lang, constraints=list(constraints_sorted) or None)\n",
    "    model_name = _model_name_for_rag()\n",
    "\n",
    "    def _call_openai(prompt: str) -> str:\n",
    "        # Records usage/latency if CURRENT_METER is set in the notebook\n",
    "        return openai_chat_with_usage(model_name, system_prompt, prompt)\n",
    "\n",
    "    # 3) Call with retries\n",
    "    raw = None\n",
    "    for attempt in range(MAX_RAG_RETRIES):\n",
    "        try:\n",
    "            raw = _call_openai(user_prompt)\n",
    "            break\n",
    "        except Exception as e:\n",
    "            if attempt == MAX_RAG_RETRIES - 1:\n",
    "                print(f\"RAG call failed (final): {e}\")\n",
    "                CACHE_TRANSLATIONS[cache_key] = \"[RAG_TRANSLATION_ERROR]\"\n",
    "                return CACHE_TRANSLATIONS[cache_key]\n",
    "            _backoff_sleep(attempt)\n",
    "\n",
    "    # 4) Extract translation block\n",
    "    translation = _extract_translation_block(raw)\n",
    "\n",
    "    # 5) Validate tag fidelity; one corrective retry if needed\n",
    "    if not tags_preserved(src_text, translation):\n",
    "        retry_prompt = user_prompt.replace(\n",
    "            \"<translation>\",\n",
    "            \"IMPORTANT: Copy every HTML tag exactly as in the source. Only the translation.\\n\\n<translation>\"\n",
    "        )\n",
    "        try:\n",
    "            raw2 = _call_openai(retry_prompt)\n",
    "            translation2 = _extract_translation_block(raw2)\n",
    "            if tags_preserved(src_text, translation2):\n",
    "                translation = translation2\n",
    "        except Exception:\n",
    "            pass  # keep the first translation even if retry fails\n",
    "\n",
    "    # 6) Cache + return\n",
    "    CACHE_TRANSLATIONS[cache_key] = translation\n",
    "    return translation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. End-to-End RAG Batch T9N Pipeline Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pipeline configuration:\n",
      " Languages: fr, ja, it\n",
      " Segments: 76\n",
      " Max workers: 4\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "import os, json, time\n",
    "from pathlib import Path\n",
    "from typing import Dict, List, Tuple, Any\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Config\n",
    "TARGET_LANGUAGES = [\"fr\", \"ja\", \"it\"]\n",
    "LANGUAGE_NAMES = {\"fr\": \"French\", \"ja\": \"Japanese\", \"it\": \"Italian\"}\n",
    "MAX_WORKERS = max(1, int(os.getenv(\"RAG_MAX_WORKERS\", \"4\"))) \n",
    "MODEL_NAME = (globals().get(\"RAG_MODEL\", {}).get(\"name\") or \"gpt-4o-mini\")\n",
    "\n",
    "print(\"Pipeline configuration:\")\n",
    "print(f\" Languages: {', '.join(TARGET_LANGUAGES)}\")\n",
    "print(f\" Segments: {len(en_segments)}\")\n",
    "print(f\" Max workers: {MAX_WORKERS}\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "\n",
    "# RAG translation over all segments for a single target language\n",
    "def translate_all_segments_rag(target_lang: str) -> List[Dict[str, Any]]:\n",
    "    start = time.time()\n",
    "\n",
    "    # 1) Deduplicate by source text (preserve stable order for reproducibility)\n",
    "    src_to_paths: Dict[str, List[str]] = {}\n",
    "    for path, src in en_segments:\n",
    "        src_to_paths.setdefault(src, []).append(path)\n",
    "    unique_srcs = sorted(src_to_paths.keys())  # stable, deterministic order\n",
    "    uniq_count = len(unique_srcs)\n",
    "\n",
    "    # 2) Precompute constraints and TM flags once per-unique source\n",
    "    src_to_constraints: Dict[str, List[str]] = {}\n",
    "    src_tm_hit: Dict[str, bool] = {}\n",
    "    total_constraints_used = 0\n",
    "    for src in unique_srcs:\n",
    "        cons = build_constraints(src, target_lang, top_k=3)\n",
    "        src_to_constraints[src] = cons\n",
    "        total_constraints_used += len(cons)\n",
    "        src_tm_hit[src] = bool(tm_lookup(src, target_lang))\n",
    "    tm_hits = sum(1 for v in src_tm_hit.values() if v)\n",
    "\n",
    "    # 3) Translate unique sources (sequential if MAX_WORKERS=1; otherwise threaded)\n",
    "    results_map: Dict[str, str] = {}\n",
    "\n",
    "    if MAX_WORKERS == 1:\n",
    "        # Sequential path: friendlier to rate limits and easier to debug\n",
    "        for src in tqdm(unique_srcs, total=uniq_count, desc=f\"RAG → {target_lang} (seq)\"):\n",
    "            try:\n",
    "                results_map[src] = translate_segment_with_rag(src, target_lang, src_to_constraints[src])\n",
    "            except Exception as e:\n",
    "                snippet = (src[:60] + \"…\") if len(src) > 60 else src\n",
    "                print(f\"Translate failed: {e} | src≈ {snippet!r}\")\n",
    "                results_map[src] = \"[RAG_TRANSLATION_ERROR]\"\n",
    "    else:\n",
    "        # Threaded path: faster but be mindful of provider rate limits\n",
    "        futures = {}\n",
    "        try:\n",
    "            with ThreadPoolExecutor(max_workers=MAX_WORKERS) as ex:\n",
    "                for src in unique_srcs:\n",
    "                    fut = ex.submit(translate_segment_with_rag, src, target_lang, src_to_constraints[src])\n",
    "                    futures[fut] = src\n",
    "\n",
    "                for fut in tqdm(as_completed(futures), total=len(futures), desc=f\"RAG → {target_lang}\"):\n",
    "                    src = futures[fut]\n",
    "                    try:\n",
    "                        results_map[src] = fut.result()\n",
    "                    except Exception as e:\n",
    "                        snippet = (src[:60] + \"…\") if len(src) > 60 else src\n",
    "                        print(f\"⚠️ Translate failed: {e} | src≈ {snippet!r}\")\n",
    "                        results_map[src] = \"[RAG_TRANSLATION_ERROR]\"\n",
    "        except KeyboardInterrupt:\n",
    "            # Best-effort cleanup on interrupt\n",
    "            for fut in futures:\n",
    "                fut.cancel()\n",
    "            raise\n",
    "\n",
    "    # 4) Expand back to all original segments (preserve original order)\n",
    "    out: List[Dict[str, Any]] = []\n",
    "    now = time.strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "    for path, src in en_segments:\n",
    "        constraints = src_to_constraints.get(src, [])\n",
    "        out.append({\n",
    "            \"path\": path,\n",
    "            \"source\": src,\n",
    "            \"translation\": results_map.get(src, \"[RAG_TRANSLATION_ERROR]\"),\n",
    "            \"model\": MODEL_NAME,\n",
    "            \"approach\": \"RAG\",\n",
    "            \"target_lang\": target_lang,\n",
    "            \"tm_hit\": src_tm_hit.get(src, False),\n",
    "            \"constraints_found\": len(constraints),\n",
    "            \"constraints_list\": constraints,\n",
    "            \"timestamp\": now,\n",
    "            **({\"source_sha\": SOURCE_SHA} if SOURCE_SHA else {}),\n",
    "        })\n",
    "\n",
    "    # 5) Save + logging\n",
    "    dur = time.time() - start\n",
    "    seg_per_sec = len(out) / max(dur, 1e-6)\n",
    "\n",
    "    out_dir = Path(\"translations/rag\")\n",
    "    out_dir.mkdir(parents=True, exist_ok=True)\n",
    "    out_file = out_dir / f\"{target_lang}.json\"\n",
    "    with open(out_file, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(out, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "    print(f\"RAG translation completed: {target_lang}\")\n",
    "    print(f\"   • Unique sources: {uniq_count}  | Segments: {len(out)}\")\n",
    "    print(f\"   • Duration: {dur:.1f}s  | Speed: {seg_per_sec:.2f} seg/s\")\n",
    "    print(f\"   • TM hits (unique-src): {tm_hits}/{uniq_count} ({tm_hits/max(uniq_count,1):.1%})\")\n",
    "    print(f\"   • Constraints found (unique-src): {total_constraints_used}\")\n",
    "    print(f\"   • Saved: {out_file}\")\n",
    "\n",
    "    return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Execute RAG T9N"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Starting French (fr)…\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "RAG → fr: 100%|████████████████████████████████████████████████████████████████████████| 73/73 [00:24<00:00,  3.02it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RAG translation completed: fr\n",
      "   • Unique sources: 73  | Segments: 76\n",
      "   • Duration: 39.4s  | Speed: 1.93 seg/s\n",
      "   • TM hits (unique-src): 2/73 (2.7%)\n",
      "   • Constraints found (unique-src): 203\n",
      "   • Saved: translations\\rag\\fr.json\n",
      "\n",
      "Starting Japanese (ja)…\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "RAG → ja: 100%|████████████████████████████████████████████████████████████████████████| 73/73 [00:26<00:00,  2.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RAG translation completed: ja\n",
      "   • Unique sources: 73  | Segments: 76\n",
      "   • Duration: 35.1s  | Speed: 2.17 seg/s\n",
      "   • TM hits (unique-src): 2/73 (2.7%)\n",
      "   • Constraints found (unique-src): 203\n",
      "   • Saved: translations\\rag\\ja.json\n",
      "\n",
      "Starting Italian (it)…\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "RAG → it: 100%|████████████████████████████████████████████████████████████████████████| 73/73 [00:26<00:00,  2.79it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RAG translation completed: it\n",
      "   • Unique sources: 73  | Segments: 76\n",
      "   • Duration: 42.5s  | Speed: 1.79 seg/s\n",
      "   • TM hits (unique-src): 2/73 (2.7%)\n",
      "   • Constraints found (unique-src): 203\n",
      "   • Saved: translations\\rag\\it.json\n",
      "\n",
      "======================================================================\n",
      "RAG TRANSLATIONS COMPLETE\n",
      "======================================================================\n",
      "French (fr) — saved: translations\\rag\\fr.json\n",
      "Japanese (ja) — saved: translations\\rag\\ja.json\n",
      "Italian (it) — saved: translations\\rag\\it.json\n",
      "\n",
      "Run summary saved to: eval\\rag_run_summary.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "from pathlib import Path\n",
    "from typing import Dict, List, Any\n",
    "import pandas as pd\n",
    "\n",
    "RAG_RESULTS: Dict[str, List[Dict[str, Any]]] = {}\n",
    "RAG_SUMMARY: Dict[str, Any] = {}\n",
    "RAG_METERS:  Dict[str, Dict[str, float]] = {}\n",
    "summary_rows: List[Dict[str, Any]] = []\n",
    "\n",
    "for lang in TARGET_LANGUAGES:\n",
    "    print(f\"\\nStarting {LANGUAGE_NAMES[lang]} ({lang})…\")\n",
    "\n",
    "    # Start a fresh usage meter per language if metering is available\n",
    "    if \"UsageMeter\" in globals():\n",
    "        CURRENT_METER = UsageMeter()  # type: ignore[name-defined]\n",
    "    else:\n",
    "        CURRENT_METER = None  # metering disabled\n",
    "\n",
    "    t0 = time.time()\n",
    "    try:\n",
    "        # Run translation (writes translations/rag/{lang}.json internally)\n",
    "        res = translate_all_segments_rag(lang)\n",
    "        dt = max(time.time() - t0, 1e-9)\n",
    "\n",
    "        # Collect usage if available\n",
    "        usage = {\"input_tokens\": 0, \"output_tokens\": 0, \"wall_seconds\": 0.0}\n",
    "        if CURRENT_METER is not None and hasattr(CURRENT_METER, \"snapshot\"):\n",
    "            usage = CURRENT_METER.snapshot()  # type: ignore[assignment]\n",
    "\n",
    "        RAG_METERS[lang] = dict(usage)\n",
    "        RAG_RESULTS[lang] = res\n",
    "\n",
    "        # Aggregate quick metrics (kept minimal; detailed eval happens later)\n",
    "        total_segments   = len(res)\n",
    "        unique_sources   = len({r.get(\"source\") for r in res})\n",
    "        tm_hits          = sum(1 for r in res if r.get(\"tm_hit\"))\n",
    "        errors           = sum(1 for r in res if \"[RAG_TRANSLATION_ERROR]\" in (r.get(\"translation\") or \"\"))\n",
    "        total_constraints= sum(int(r.get(\"constraints_found\", 0)) for r in res)\n",
    "        out_file         = Path(\"translations/rag\") / f\"{lang}.json\"\n",
    "\n",
    "        RAG_SUMMARY[lang] = {\n",
    "            \"language\": lang,\n",
    "            \"total_segments\": total_segments,\n",
    "            \"unique_sources\": unique_sources,\n",
    "            \"tm_hits_segments\": tm_hits,\n",
    "            \"tm_hit_rate_segments\": (tm_hits / total_segments) if total_segments else 0.0,\n",
    "            \"errors\": errors,\n",
    "            \"error_rate\": (errors / total_segments) if total_segments else 0.0,\n",
    "            \"total_constraints\": total_constraints,\n",
    "            \"avg_constraints_per_segment\": (total_constraints / total_segments) if total_segments else 0.0,\n",
    "            \"duration_sec\": round(dt, 2),\n",
    "            \"segments_per_sec\": (total_segments / dt) if dt > 0 else 0.0,\n",
    "            \"output_file\": str(out_file),\n",
    "            \"input_tokens\": int(usage.get(\"input_tokens\", 0)),\n",
    "            \"output_tokens\": int(usage.get(\"output_tokens\", 0)),\n",
    "            \"wall_seconds_metered\": float(usage.get(\"wall_seconds\", 0.0)),\n",
    "        }\n",
    "        summary_rows.append(RAG_SUMMARY[lang])\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Failed {LANGUAGE_NAMES[lang]}: {e}\")\n",
    "        RAG_RESULTS[lang] = []\n",
    "        err_row = {\"error\": str(e), \"language\": lang}\n",
    "        RAG_SUMMARY[lang] = err_row\n",
    "        summary_rows.append(err_row)\n",
    "\n",
    "# Concise recap (details & charts are handled in later evaluation cells)\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"RAG TRANSLATIONS COMPLETE\")\n",
    "print(\"=\" * 70)\n",
    "for lang in TARGET_LANGUAGES:\n",
    "    s = RAG_SUMMARY.get(lang, {})\n",
    "    if \"error\" in s:\n",
    "        print(f\"{LANGUAGE_NAMES[lang]} ({lang}): {s['error']}\")\n",
    "    else:\n",
    "        print(f\"{LANGUAGE_NAMES[lang]} ({lang}) — saved: {s['output_file']}\")\n",
    "\n",
    "# Persist a one-line-per-language summary for downstream analysis\n",
    "Path(\"eval\").mkdir(parents=True, exist_ok=True)\n",
    "summary_path = Path(\"eval/rag_run_summary.csv\")\n",
    "pd.DataFrame(summary_rows).to_csv(summary_path, index=False)\n",
    "print(f\"\\nRun summary saved to: {summary_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Baseline Loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Baseline loader\n",
    "# gpt-4o-mini was selected in phase one of the project as the baseline.\n",
    "\n",
    "from pathlib import Path\n",
    "from typing import Dict, List, Optional, Tuple\n",
    "import os, json\n",
    "\n",
    "_PREFERRED_TOKENS = tuple(os.getenv(\"BASELINE_MODEL_HINT\", \"gpt-4o-mini gpt openai claude gemini\").split())\n",
    "\n",
    "def _choose_baseline_dir(\n",
    "    base_dir: Path,\n",
    "    langs: List[str],\n",
    "    prefer: Tuple[str, ...] = _PREFERRED_TOKENS,\n",
    ") -> Optional[Path]:\n",
    "    \"\"\"\n",
    "    Pick a subdirectory under `base_dir` that contains {lang}.json for all langs.\n",
    "    Tie-break by:\n",
    "      (1) folder name contains any token in `prefer` (earlier token = higher score)\n",
    "      (2) most recent mtime\n",
    "    Returns Path or None.\n",
    "    \"\"\"\n",
    "    if not base_dir.exists():\n",
    "        return None\n",
    "\n",
    "    candidates: List[Tuple[int, float, Path]] = []\n",
    "    for d in sorted(base_dir.iterdir(), key=lambda p: p.name.lower()):\n",
    "        if not d.is_dir():\n",
    "            continue\n",
    "        if not all((d / f\"{lang}.json\").exists() for lang in langs):\n",
    "            continue\n",
    "\n",
    "        dn = d.name.lower()\n",
    "        pref_score = 0\n",
    "        for i, tok in enumerate(prefer):\n",
    "            if tok and tok.lower() in dn:\n",
    "                pref_score = max(pref_score, len(prefer) - i)\n",
    "        candidates.append((pref_score, d.stat().st_mtime, d))\n",
    "\n",
    "    if not candidates:\n",
    "        return None\n",
    "\n",
    "    candidates.sort(key=lambda t: (t[0], t[1]), reverse=True)\n",
    "    return candidates[0][2]\n",
    "\n",
    "\n",
    "def load_baseline_results(\n",
    "    target_languages: Optional[List[str]] = None,\n",
    "    *,\n",
    "    base_dir: Path = Path(\"translations/baseline\"),\n",
    "    prefer_hint_env: str = \"BASELINE_MODEL_HINT\",\n",
    "    verbose: bool = True,\n",
    ") -> Dict[str, List[Dict]]:\n",
    "    \"\"\"\n",
    "    Load baseline translations from `translations/baseline/<chosen>/`.\n",
    "    - Chooses a folder that has {lang}.json for all requested languages.\n",
    "    - Prefers directories whose name contains tokens in BASELINE_MODEL_HINT\n",
    "      (default: \"gpt-4o-mini gpt openai claude gemini\").\n",
    "    Returns {lang: [rows]} where each row has: path, source, translation.\n",
    "    \"\"\"\n",
    "    langs = target_languages or globals().get(\"TARGET_LANGUAGES\", [])\n",
    "    if not langs:\n",
    "        if verbose:\n",
    "            print(\"No target languages provided and TARGET_LANGUAGES not defined.\")\n",
    "        return {}\n",
    "\n",
    "    prefer_tokens = tuple(os.getenv(prefer_hint_env, \"gpt-4o-mini gpt openai claude gemini\").split())\n",
    "    chosen = _choose_baseline_dir(base_dir, langs, prefer=prefer_tokens)\n",
    "    if not chosen:\n",
    "        if verbose:\n",
    "            print(f\"No baseline folder with all languages {langs} under: {base_dir}\")\n",
    "        return {}\n",
    "\n",
    "    if verbose:\n",
    "        print(f\"Loading baseline from: {chosen}\")\n",
    "\n",
    "    out: Dict[str, List[Dict]] = {}\n",
    "    for lang in langs:\n",
    "        f = chosen / f\"{lang}.json\"\n",
    "        try:\n",
    "            with f.open(\"r\", encoding=\"utf-8\") as fh:\n",
    "                out[lang] = json.load(fh)\n",
    "            if verbose:\n",
    "                print(f\"  • {lang}: {len(out[lang])} segments\")\n",
    "        except Exception as e:\n",
    "            if verbose:\n",
    "                print(f\"  Failed to load {lang} from {f}: {e}\")\n",
    "            out[lang] = []\n",
    "    return out\n",
    "\n",
    "\n",
    "# Baseline Metrics loader\n",
    "def _choose_baseline_metrics_dir(\n",
    "    base_dir: Path = Path(\"eval/baseline\"),\n",
    "    langs: Optional[List[str]] = None,\n",
    "    prefer: Tuple[str, ...] = _PREFERRED_TOKENS,\n",
    ") -> Optional[Path]:\n",
    "    \"\"\"\n",
    "    Pick a subdirectory under `eval/baseline/` that contains metrics_{lang}.json\n",
    "    for all langs. Prefers directories matching tokens in `prefer`, then newest.\n",
    "    \"\"\"\n",
    "    if not base_dir.exists():\n",
    "        return None\n",
    "    langs = langs or globals().get(\"TARGET_LANGUAGES\", [])\n",
    "    if not langs:\n",
    "        return None\n",
    "\n",
    "    candidates: List[Tuple[int, float, Path]] = []\n",
    "    for d in sorted(base_dir.iterdir(), key=lambda p: p.name.lower()):\n",
    "        if not d.is_dir():\n",
    "            continue\n",
    "        if not all((d / f\"metrics_{lang}.json\").exists() for lang in langs):\n",
    "            continue\n",
    "\n",
    "        dn = d.name.lower()\n",
    "        pref_score = 0\n",
    "        for i, tok in enumerate(prefer):\n",
    "            if tok and tok.lower() in dn:\n",
    "                pref_score = max(pref_score, len(prefer) - i)\n",
    "        candidates.append((pref_score, d.stat().st_mtime, d))\n",
    "\n",
    "    if not candidates:\n",
    "        return None\n",
    "\n",
    "    candidates.sort(key=lambda t: (t[0], t[1]), reverse=True)\n",
    "    return candidates[0][2]\n",
    "\n",
    "\n",
    "def load_baseline_eval_metrics(\n",
    "    *,\n",
    "    base_dir: Path = Path(\"eval/baseline\"),\n",
    "    prefer_hint_env: str = \"BASELINE_MODEL_HINT\",\n",
    "    verbose: bool = True,\n",
    ") -> Dict[str, dict]:\n",
    "    \"\"\"\n",
    "    Load baseline metrics from `eval/baseline/<chosen>/metrics_{lang}.json`.\n",
    "    Returns {lang: {...metrics...}}. Prefers `gpt-4o-mini` unless overridden by env.\n",
    "    \"\"\"\n",
    "    langs = globals().get(\"TARGET_LANGUAGES\", [])\n",
    "    if not langs:\n",
    "        if verbose:\n",
    "            print(\"TARGET_LANGUAGES not defined; cannot load baseline metrics.\")\n",
    "        return {}\n",
    "\n",
    "    prefer_tokens = tuple(os.getenv(prefer_hint_env, \"gpt-4o-mini gpt openai claude gemini\").split())\n",
    "    chosen = _choose_baseline_metrics_dir(base_dir=base_dir, langs=langs, prefer=prefer_tokens)\n",
    "    if not chosen:\n",
    "        if verbose:\n",
    "            print(f\"No baseline metrics folder found under: {base_dir}\")\n",
    "        return {}\n",
    "\n",
    "    if verbose:\n",
    "        print(f\"Using baseline eval metrics from: {chosen}\")\n",
    "\n",
    "    out: Dict[str, dict] = {}\n",
    "    for lang in langs:\n",
    "        p = chosen / f\"metrics_{lang}.json\"\n",
    "        try:\n",
    "            with p.open(\"r\", encoding=\"utf-8\") as fh:\n",
    "                out[lang] = json.load(fh)\n",
    "        except Exception as e:\n",
    "            if verbose:\n",
    "                print(f\"  couldn't read {p}: {e}\")\n",
    "            out[lang] = {}\n",
    "    return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Quality Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "RAG EVALUATION (compact)\n",
      "============================================================\n",
      "Loading baseline from: translations\\baseline\\gpt-4o-mini\n",
      "  • fr: 76 segments\n",
      "  • ja: 76 segments\n",
      "  • it: 76 segments\n",
      "Using baseline eval metrics from: eval\\baseline\\gpt-4o-mini\n",
      "\n",
      "French (FR)\n",
      "----------------------------------------\n",
      "  DNT 100.0% | Glossary 96.1% | Tags 97.4% | Ret@k 18.4%\n",
      "  Semantic similarity (vs baseline): 0.980\n",
      "  RAG  : 39.40s | 1.93 seg/s\n",
      "  Base : 32.69s | 2.33 seg/s\n",
      "\n",
      "Japanese (JA)\n",
      "----------------------------------------\n",
      "  DNT 100.0% | Glossary 99.3% | Tags 97.4% | Ret@k 18.4%\n",
      "  Semantic similarity (vs baseline): 0.977\n",
      "  RAG  : 35.09s | 2.17 seg/s\n",
      "  Base : 22.81s | 3.33 seg/s\n",
      "\n",
      "Italian (IT)\n",
      "----------------------------------------\n",
      "  DNT 100.0% | Glossary 95.6% | Tags 97.4% | Ret@k 18.4%\n",
      "  Semantic similarity (vs baseline): 0.983\n",
      "  RAG  : 42.48s | 1.79 seg/s\n",
      "  Base : 21.91s | 3.47 seg/s\n",
      "\n",
      "Saved: eval\\rag_comprehensive_evaluation.csv\n",
      "Saved: eval\\rag_quality_vs_baseline.csv\n"
     ]
    }
   ],
   "source": [
    "from __future__ import annotations\n",
    "from pathlib import Path\n",
    "from typing import Dict, List, Optional, Tuple, Any\n",
    "import json, os, re, time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Helpers\n",
    "_HTML_TAG = re.compile(r\"</?\\w+(?:\\s+[^>]*?)?>\", re.IGNORECASE)\n",
    "\n",
    "def tags_preserved(src: str, tgt: str) -> bool:\n",
    "    return _HTML_TAG.findall(src or \"\") == _HTML_TAG.findall(tgt or \"\")\n",
    "\n",
    "def dnt_preserved(src: str, tgt: str) -> bool:\n",
    "    s, t = (src or \"\"), (tgt or \"\")\n",
    "    for term in DNT_TERMS:\n",
    "        if term in s and s.count(term) != t.count(term):\n",
    "            return False\n",
    "    return True\n",
    "\n",
    "def glossary_adherence(src: str, tgt: str, lang: str) -> float:\n",
    "    gm = GLOSSARY_MAP.get(lang, {})\n",
    "    if not gm:\n",
    "        return 1.0\n",
    "    s, t = (src or \"\"), (tgt or \"\")\n",
    "    checks = hits = 0\n",
    "    for en_term, target_term in gm.items():\n",
    "        if en_term and target_term and en_term in s:\n",
    "            checks += 1\n",
    "            if target_term in t:\n",
    "                hits += 1\n",
    "    return (hits / checks) if checks else 1.0\n",
    "\n",
    "def retrieval_precision(cands: List[str], src: str) -> float:\n",
    "    if not cands:\n",
    "        return 1.0\n",
    "    s = (src or \"\").lower()\n",
    "    tp = sum(1 for t in cands if (t or \"\").lower() in s)\n",
    "    return tp / max(len(cands), 1)\n",
    "\n",
    "def _fmt(x, unit=\"\"):\n",
    "    return \"n/a\" if x is None else f\"{x:.2f}{unit}\"\n",
    "\n",
    "# Optional: thin shim so we can call a public loader if it exists\n",
    "def _try_load_baseline_eval_metrics() -> Dict[str, dict]:\n",
    "    # Prefer the public helper if you pasted it elsewhere\n",
    "    if \"load_baseline_eval_metrics\" in globals() and callable(globals()[\"load_baseline_eval_metrics\"]):\n",
    "        return globals()[\"load_baseline_eval_metrics\"]()\n",
    "    # Fallback to newest metrics folder\n",
    "    root = Path(\"eval/baseline\")\n",
    "    if not root.exists():\n",
    "        return {}\n",
    "    cands = [d for d in root.iterdir() if d.is_dir()]\n",
    "    cands = [d for d in cands if all((d / f\"metrics_{lang}.json\").exists() for lang in TARGET_LANGUAGES)]\n",
    "    if not cands:\n",
    "        return {}\n",
    "    chosen = max(cands, key=lambda p: p.stat().st_mtime)\n",
    "    out: Dict[str, dict] = {}\n",
    "    for lang in TARGET_LANGUAGES:\n",
    "        try:\n",
    "            with open(chosen / f\"metrics_{lang}.json\", \"r\", encoding=\"utf-8\") as fh:\n",
    "                out[lang] = json.load(fh)\n",
    "        except Exception:\n",
    "            out[lang] = {}\n",
    "    print(f\"Using baseline eval metrics from: {chosen}\")\n",
    "    return out\n",
    "\n",
    "# -- main ----------------------------------------------------------------------\n",
    "def evaluate_rag_compact():\n",
    "    print(\"\\nRAG EVALUATION (compact)\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "    # Baseline translations: dict or (dict, path)\n",
    "    _loaded = load_baseline_results()\n",
    "    if isinstance(_loaded, tuple):\n",
    "        baseline_results, _ = _loaded\n",
    "    else:\n",
    "        baseline_results = _loaded or {}\n",
    "\n",
    "    baseline_eval = _try_load_baseline_eval_metrics()\n",
    "\n",
    "    results_rows: List[Dict[str, Any]] = []\n",
    "    compare_rows: List[Dict[str, Any]] = []\n",
    "\n",
    "    rag_model_name = (\n",
    "        (OPENAI_MODELS.get(\"gpt-4o-mini\", {}).get(\"model\") if \"OPENAI_MODELS\" in globals() else None)\n",
    "        or globals().get(\"MODEL_NAME\")\n",
    "        or os.getenv(\"OPENAI_RAG_MODEL\", \"gpt-4o-mini\")\n",
    "    )\n",
    "\n",
    "    for lang in TARGET_LANGUAGES:\n",
    "        rag = RAG_RESULTS.get(lang) or []\n",
    "        if not rag:\n",
    "            print(f\"- No RAG results for {lang}\")\n",
    "            continue\n",
    "\n",
    "        base = (baseline_results or {}).get(lang, [])\n",
    "        base_lookup = {x.get(\"path\"): x.get(\"translation\", \"\") for x in base if \"path\" in x}\n",
    "\n",
    "        seg_rows: List[Dict[str, Any]] = []\n",
    "        rag_hyps, base_refs = [], []\n",
    "\n",
    "        for it in rag:\n",
    "            src = it.get(\"source\", \"\")\n",
    "            tgt = it.get(\"translation\", \"\")\n",
    "            path = it.get(\"path\", \"\")\n",
    "            retrieved = it.get(\"constraints_list\", []) or []\n",
    "\n",
    "            seg_rows.append({\n",
    "                \"dnt\":   dnt_preserved(src, tgt),\n",
    "                \"gloss\": glossary_adherence(src, tgt, lang),\n",
    "                \"tags\":  tags_preserved(src, tgt),\n",
    "                \"retp\":  retrieval_precision([t.split(\" → \")[0] for t in retrieved], src),\n",
    "                \"err\":   \"[RAG_TRANSLATION_ERROR]\" in (tgt or \"\"),\n",
    "            })\n",
    "\n",
    "            if path in base_lookup:\n",
    "                rag_hyps.append(tgt or \"\")\n",
    "                base_refs.append(base_lookup[path] or \"\")\n",
    "\n",
    "        tot = len(seg_rows)\n",
    "        if tot == 0:\n",
    "            print(f\"- No evaluable segments for {lang}\")\n",
    "            continue\n",
    "\n",
    "        dnt_rate  = sum(r[\"dnt\"]   for r in seg_rows) / tot\n",
    "        gloss_avg = sum(r[\"gloss\"] for r in seg_rows) / tot\n",
    "        tag_rate  = sum(r[\"tags\"]  for r in seg_rows) / tot\n",
    "        retp_avg  = sum(r[\"retp\"]  for r in seg_rows) / tot\n",
    "        err_rate  = sum(r[\"err\"]   for r in seg_rows) / tot\n",
    "\n",
    "        sem_sim = None\n",
    "        if rag_hyps and base_refs:\n",
    "            try:\n",
    "                h = EMB_MODEL.encode(rag_hyps, batch_size=64, normalize_embeddings=True)\n",
    "                b = EMB_MODEL.encode(base_refs, batch_size=64, normalize_embeddings=True)\n",
    "                sims = (h * b).sum(axis=1).astype(float)\n",
    "                sem_sim = float(np.clip(sims, -1.0, 1.0).mean())\n",
    "            except Exception as e:\n",
    "                print(f\"  (semantic sim failed for {lang}: {e})\")\n",
    "\n",
    "        rag_lat   = (RAG_SUMMARY.get(lang) or {}).get(\"duration_sec\")\n",
    "        rag_speed = (RAG_SUMMARY.get(lang) or {}).get(\"segments_per_sec\")\n",
    "\n",
    "        base_lat = base_speed = None\n",
    "        if lang in baseline_eval:\n",
    "            base_lat   = baseline_eval[lang].get(\"total_duration_sec\")\n",
    "            spm        = baseline_eval[lang].get(\"segments_per_minute\")\n",
    "            if isinstance(spm, (int, float)) and spm:\n",
    "                base_speed = round(spm / 60.0, 4)\n",
    "\n",
    "        results_rows.append({\n",
    "            \"language\": lang,\n",
    "            \"total_segments\": tot,\n",
    "            \"dnt_preservation_rate\": round(dnt_rate, 3),\n",
    "            \"glossary_adherence_avg\": round(gloss_avg, 3),\n",
    "            \"tag_preservation_rate\": round(tag_rate, 3),\n",
    "            \"retrieval_precision_avg\": round(retp_avg, 3),\n",
    "            \"semantic_similarity_avg\": round(sem_sim, 3) if sem_sim is not None else None,\n",
    "            \"error_rate\": round(err_rate, 3),\n",
    "        })\n",
    "\n",
    "        compare_rows.append({\n",
    "            \"language\": lang,\n",
    "            \"rag_model\": rag_model_name,\n",
    "            \"rag_duration_sec\": rag_lat,\n",
    "            \"rag_segments_per_sec\": rag_speed,\n",
    "            \"baseline_duration_sec\": base_lat,\n",
    "            \"baseline_segments_per_sec\": base_speed,\n",
    "        })\n",
    "\n",
    "        print(f\"\\n{LANGUAGE_NAMES[lang]} ({lang.upper()})\")\n",
    "        print(\"-\" * 40)\n",
    "        print(f\"  DNT {dnt_rate:.1%} | Glossary {gloss_avg:.1%} | Tags {tag_rate:.1%} | Ret@k {retp_avg:.1%}\")\n",
    "        if sem_sim is not None:\n",
    "            print(f\"  Semantic similarity (vs baseline): {sem_sim:.3f}\")\n",
    "        print(f\"  RAG  : { _fmt(rag_lat,'s')} | { _fmt(rag_speed,' seg/s')}\")\n",
    "        print(f\"  Base : { _fmt(base_lat,'s')} | { _fmt(base_speed,' seg/s')}\")\n",
    "\n",
    "    # Save CSVs\n",
    "    out_dir = Path(\"eval\"); out_dir.mkdir(parents=True, exist_ok=True)\n",
    "    rag_df = pd.DataFrame(results_rows)\n",
    "    cmp_df = pd.DataFrame(compare_rows)\n",
    "    if not rag_df.empty:\n",
    "        rag_df.to_csv(out_dir / \"rag_comprehensive_evaluation.csv\", index=False)\n",
    "        print(f\"\\nSaved: {out_dir / 'rag_comprehensive_evaluation.csv'}\")\n",
    "    if not cmp_df.empty:\n",
    "        cmp_df.to_csv(out_dir / \"rag_quality_vs_baseline.csv\", index=False)\n",
    "        print(f\"Saved: {out_dir / 'rag_quality_vs_baseline.csv'}\")\n",
    "\n",
    "    return (rag_df if not rag_df.empty else None,\n",
    "            cmp_df if not cmp_df.empty else None)\n",
    "\n",
    "# Execute\n",
    "RAG_DF, RAG_VS_BASELINE_DF = evaluate_rag_compact()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
