{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LLM Translation Baseline + Evaluation\n",
    "\n",
    "This notebook evaluates baseline translation quality across multiple LLM models:\n",
    "- **Gemini 2.5 Flash**\n",
    "- **Claude 3.5 Sonnet**  \n",
    "- **GPT-4o Mini**\n",
    "\n",
    "**Target Languages:** French, Italian, Japanese\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Environment Setup & Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LLM Translation Baseline + Evaluation ready!\n",
      "========================================================================\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import json\n",
    "import time\n",
    "import random\n",
    "import warnings\n",
    "from pathlib import Path\n",
    "from typing import Any, List, Tuple, Dict, Optional\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from dotenv import load_dotenv\n",
    "from tqdm import tqdm\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "pd.set_option(\"display.max_columns\", None)\n",
    "pd.set_option(\"display.width\", None)\n",
    "\n",
    "load_dotenv()\n",
    "print(\"LLM Translation Baseline + Evaluation ready!\")\n",
    "print(\"=\" * 72)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Configs & Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "TARGET_LANGUAGES = [\"fr\", \"ja\", \"it\"]\n",
    "LANGUAGE_NAMES = {\"fr\": \"French\", \"ja\": \"Japanese\", \"it\": \"Italian\"}\n",
    "\n",
    "# Concurrency\n",
    "MAX_WORKERS = int(os.getenv(\"BASELINE_MAX_WORKERS\", \"4\"))\n",
    "MAX_RETRIES = int(os.getenv(\"BASELINE_MAX_RETRIES\", \"3\"))\n",
    "\n",
    "# Pricing\n",
    "PRICING_PER_1M_TOKENS = {\n",
    "    \"gemini-2.5-flash\": {\"input\": float(os.getenv(\"PRICE_GEMINI_IN\", 0.075)), \"output\": float(os.getenv(\"PRICE_GEMINI_OUT\", 0.30))},\n",
    "    \"gpt-4o-mini\": {\"input\": float(os.getenv(\"PRICE_O4MINI_IN\", 0.15)), \"output\": float(os.getenv(\"PRICE_O4MINI_OUT\", 0.60))},\n",
    "    \"claude-3-5-sonnet\": {\"input\": float(os.getenv(\"PRICE_SONNET_IN\", 3.00)), \"output\": float(os.getenv(\"PRICE_SONNET_OUT\", 15.00))},\n",
    "}\n",
    "\n",
    "OUT_BASE = Path(\"translations/baseline\")\n",
    "EVAL_BASE = Path(\"eval/baseline\")\n",
    "OUT_BASE.mkdir(parents=True, exist_ok=True)\n",
    "EVAL_BASE.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Per-provider safe concurrency\n",
    "PROVIDER_MAX_WORKERS = {\n",
    "    \"openai\":    int(os.getenv(\"OPENAI_MAX_WORKERS\", \"4\")),\n",
    "    \"anthropic\": int(os.getenv(\"ANTHROPIC_MAX_WORKERS\", \"1\")),  # Claude: keep low because kept running into overload issues\n",
    "    \"google\":    int(os.getenv(\"GOOGLE_MAX_WORKERS\", \"2\")),\n",
    "}\n",
    "\n",
    "def get_max_workers(provider: str, default: int = 2) -> int:\n",
    "    return max(1, PROVIDER_MAX_WORKERS.get(provider, default))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Data Loading & Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total string segments: 76\n",
      "  1. nav.design: 'Design'\n",
      "  2. nav.about: 'About Us'\n",
      "  3. nav.faq: 'FAQ'\n",
      "Unique source strings: 73 (dedup from 76)\n"
     ]
    }
   ],
   "source": [
    "def flatten_json_strings(obj: Any, prefix: str = \"\") -> List[Tuple[str, str]]:\n",
    "    out: List[Tuple[str, str]] = []\n",
    "    if isinstance(obj, dict):\n",
    "        for k, v in obj.items():\n",
    "            new_prefix = f\"{prefix}.{k}\" if prefix else k\n",
    "            out.extend(flatten_json_strings(v, new_prefix))\n",
    "    elif isinstance(obj, list):\n",
    "        for i, v in enumerate(obj):\n",
    "            new_prefix = f\"{prefix}[{i}]\"\n",
    "            out.extend(flatten_json_strings(v, new_prefix))\n",
    "    elif isinstance(obj, str) and obj.strip():\n",
    "        out.append((prefix, obj))\n",
    "    return out\n",
    "\n",
    "SRC_FILE = Path(\"data/en.json\")\n",
    "if not SRC_FILE.exists():\n",
    "    raise FileNotFoundError(\"Missing required file: data/en.json\")\n",
    "\n",
    "with open(SRC_FILE, \"r\", encoding=\"utf-8\") as f:\n",
    "    en_json = json.load(f)\n",
    "\n",
    "en_segments: List[Tuple[str, str]] = flatten_json_strings(en_json)\n",
    "print(f\"Total string segments: {len(en_segments)}\")\n",
    "if en_segments:\n",
    "    for i, (p, t) in enumerate(en_segments[:3], 1):\n",
    "        print(f\"  {i}. {p}: '{t}'\")\n",
    "\n",
    "# Build mappings for dedupe/expansion\n",
    "SRC_TO_PATHS: Dict[str, List[str]] = {}\n",
    "for path, src in en_segments:\n",
    "    SRC_TO_PATHS.setdefault(src, []).append(path)\n",
    "UNIQUE_SOURCES = list(SRC_TO_PATHS.keys())\n",
    "print(f\"Unique source strings: {len(UNIQUE_SOURCES)} (dedup from {len(en_segments)})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. LLM Client Initialization & Model Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing clients…\n",
      "Google (genai) ready\n",
      "OpenAI ready\n",
      "Anthropic ready\n",
      "Baseline models configured:\n",
      "  • gemini-2.5-flash (google): gemini-2.5-flash\n",
      "  • gpt-4o-mini (openai): gpt-4o-mini\n",
      "  • claude-3-5-sonnet (anthropic): claude-3-5-sonnet-latest\n"
     ]
    }
   ],
   "source": [
    "GOOGLE_API_KEY = os.getenv(\"GOOGLE_API_KEY\")\n",
    "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\n",
    "ANTHROPIC_API_KEY = os.getenv(\"ANTHROPIC_API_KEY\")\n",
    "\n",
    "if not (GOOGLE_API_KEY or OPENAI_API_KEY or ANTHROPIC_API_KEY):\n",
    "    raise RuntimeError(\"No API keys found! Add at least one provider key to .env\")\n",
    "\n",
    "print(\"Initializing clients…\")\n",
    "\n",
    "# Google: prefer new google.genai, fallback to google.generativeai\n",
    "GOOGLE_CLIENT = None\n",
    "GOOGLE_MODELS = {}\n",
    "if GOOGLE_API_KEY:\n",
    "    try:\n",
    "        import google.genai as genai\n",
    "        GOOGLE_CLIENT = genai.Client(api_key=GOOGLE_API_KEY)\n",
    "        GOOGLE_MODELS[\"gemini-2.5-flash\"] = {\"model\": os.getenv(\"GOOGLE_BASELINE_MODEL\", \"gemini-2.5-flash\")}\n",
    "        print(\"Google (genai) ready\")\n",
    "    except Exception as e:\n",
    "        try:\n",
    "            import google.generativeai as genai_legacy\n",
    "            genai_legacy.configure(api_key=GOOGLE_API_KEY)\n",
    "            GOOGLE_CLIENT = genai_legacy\n",
    "            GOOGLE_MODELS[\"gemini-2.5-flash\"] = {\"model\": os.getenv(\"GOOGLE_BASELINE_MODEL\", \"gemini-2.5-flash\")}\n",
    "            print(\"Using legacy google.generativeai client\")\n",
    "        except Exception as e2:\n",
    "            print(f\"Google init failed: {e} | {e2}\")\n",
    "            GOOGLE_CLIENT = None\n",
    "\n",
    "# OpenAI\n",
    "OPENAI_CLIENT = None\n",
    "OPENAI_MODELS = {}\n",
    "if OPENAI_API_KEY:\n",
    "    try:\n",
    "        from openai import OpenAI\n",
    "        OPENAI_CLIENT = OpenAI(api_key=OPENAI_API_KEY)\n",
    "        OPENAI_MODELS[\"gpt-4o-mini\"] = {\"model\": os.getenv(\"OPENAI_BASELINE_MODEL\", \"gpt-4o-mini\")}\n",
    "        print(\"OpenAI ready\")\n",
    "    except Exception as e:\n",
    "        print(f\"OpenAI init failed: {e}\")\n",
    "        OPENAI_CLIENT = None\n",
    "\n",
    "# Anthropic\n",
    "ANTHROPIC_CLIENT = None\n",
    "ANTHROPIC_MODELS = {}\n",
    "if ANTHROPIC_API_KEY:\n",
    "    try:\n",
    "        from anthropic import Anthropic\n",
    "        ANTHROPIC_CLIENT = Anthropic(api_key=ANTHROPIC_API_KEY)\n",
    "        ANTHROPIC_MODELS[\"claude-3-5-sonnet\"] = {\"model\": os.getenv(\"ANTHROPIC_BASELINE_MODEL\", \"claude-3-5-sonnet-latest\")}\n",
    "        print(\"Anthropic ready\")\n",
    "    except Exception as e:\n",
    "        print(f\"Anthropic init failed: {e}\")\n",
    "        ANTHROPIC_CLIENT = None\n",
    "\n",
    "# Compose BASELINE_MODELS registry actually available\n",
    "BASELINE_MODELS: Dict[str, Tuple[str, Dict[str, str]]] = {}\n",
    "if GOOGLE_CLIENT:\n",
    "    BASELINE_MODELS.update({\"gemini-2.5-flash\": (\"google\", GOOGLE_MODELS[\"gemini-2.5-flash\"])})\n",
    "if OPENAI_CLIENT:\n",
    "    BASELINE_MODELS.update({\"gpt-4o-mini\": (\"openai\", OPENAI_MODELS[\"gpt-4o-mini\"])})\n",
    "if ANTHROPIC_CLIENT:\n",
    "    BASELINE_MODELS.update({\"claude-3-5-sonnet\": (\"anthropic\", ANTHROPIC_MODELS[\"claude-3-5-sonnet\"])})\n",
    "\n",
    "print(\"Baseline models configured:\")\n",
    "for m, (prov, cfg) in BASELINE_MODELS.items():\n",
    "    print(f\"  • {m} ({prov}): {cfg['model']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Prompting Helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "FEWSHOT_EXAMPLES = {\n",
    "    \"fr\": [\n",
    "        (\"Each beautiful press-on nail is carefully crafted by <strong>our in-house</strong> nail techs.\",\n",
    "         \"Chaque beau press-on nail est soigneusement conçu par nos stylistes ongulaires <strong>maison</strong>.\"),\n",
    "        (\"Instead of using brittle acrylic nail bases, NaiLit uses Gel-X soft gel nail bases that provide just the right curvature and comfort for your natural nails.\",\n",
    "         \"Au lieu d’utiliser des bases d’ongles fragiles en acrylic, NaiLit utilise des bases en gel souple Gel-X qui offrent la courbure et le confort idéals pour vos ongles naturels.\"),\n",
    "        (\"Whether you feel like elegant nude French coffin nails or trendy seafoam cat eye stiletto nails, NaiLit has just the right options for you to fully customize.\",\n",
    "         \"Que vous ayez envie de nude français ballerine élégant ou de vert d’eau cat eye pointu tendance, NaiLit vous propose les options idéales pour une personnalisation totale.\"),\n",
    "    ],\n",
    "    \"ja\": [\n",
    "        (\"Each beautiful press-on nail is carefully crafted by <strong>our in-house</strong> nail techs.\",\n",
    "         \"それぞれの美しい press-on nail は、<strong>当社専属</strong>のネイリストが丁寧に仕上げています。\"),\n",
    "        (\"Instead of using brittle acrylic nail bases, NaiLit uses Gel-X soft gel nail bases that provide just the right curvature and comfort for your natural nails.\",\n",
    "         \"割れやすいアクリルベースの代わりに、NaiLitは柔らかいジェルのGel-Xベースを使用し、地爪に最適なカーブと快適さを提供します。\"),\n",
    "        (\"Whether you feel like elegant nude French coffin nails or trendy seafoam cat eye stiletto nails, NaiLit has just the right options for you to fully customize.\",\n",
    "         \"エレガントなヌーディーバレリーナフレンチネールや、トレンディなミントマグネットポイントネールのどちらの気分でも、NaiLitなら思い通りにカスタマイズできます。\"),\n",
    "    ],\n",
    "    \"it\": [\n",
    "        (\"Each beautiful press-on nail is carefully crafted by <strong>our in-house</strong> nail techs.\",\n",
    "         \"Ogni bellissima press-on nail è accuratamente realizzata dalle nostre onicotecniche <strong>della casa</strong>.\"),\n",
    "        (\"Instead of using brittle acrylic nail bases, NaiLit uses Gel-X soft gel nail bases that provide just the right curvature and comfort for your natural nails.\",\n",
    "         \"Invece di utilizzare basi per unghie fragili in acrylic, NaiLit impiega basi in gel morbido Gel-X che garantiscono la curvatura e il comfort perfetti per le tue unghie naturali.\"),\n",
    "        (\"Whether you feel like elegant nude French coffin nails or trendy seafoam cat eye stiletto nails, NaiLit has le opzioni perfette per personalizzare al massimo.\",\n",
    "         \"Che tu abbia voglia di un’elegante nude French ballerina o di un trendy verde acqua cat-eye stiletto, NaiLit ha le opzioni perfette per personalizzare al massimo.\"),\n",
    "    ],\n",
    "}\n",
    "\n",
    "LANGUAGE_NAMES_HUMAN = {\"fr\": \"French\", \"ja\": \"Japanese\", \"it\": \"Italian\"}\n",
    "\n",
    "\n",
    "PROMPT_HEADERS: Dict[str, str] = {}\n",
    "for lang, pairs in FEWSHOT_EXAMPLES.items():\n",
    "    lang_name = LANGUAGE_NAMES_HUMAN.get(lang, lang)\n",
    "    examples = \"\".join([f\"\\nExample {i}:\\nSource: {src}\\nTranslation: {tgt}\\n\" for i, (src, tgt) in enumerate(pairs, 1)])\n",
    "    PROMPT_HEADERS[lang] = (\n",
    "        f\"You are a professional UX translator. Translate the following text into {lang_name}.\\n\\n\"\n",
    "        \"IMPORTANT REQUIREMENTS:\\n\"\n",
    "        \"- Preserve all HTML tags exactly (do not add/remove/reorder tags)\\n\"\n",
    "        \"- Keep brand names and DNT terms as-is (e.g., NaiLit)\\n\"\n",
    "        \"- Maintain the original tone and style\\n\"\n",
    "        \"- Return ONLY the translation between <translation> and </translation> — no notes\\n\\n\"\n",
    "        f\"Here are some examples:{examples}\\n\\n\"\n",
    "        \"Now translate this:\\nSource: \"\n",
    "    )\n",
    "\n",
    "HTML_TAG = re.compile(r\"</?\\w+(?:\\s+[^>]*?)?>\", re.IGNORECASE)\n",
    "\n",
    "def tags_preserved(src: str, tgt: str) -> bool:\n",
    "    return HTML_TAG.findall(src or \"\") == HTML_TAG.findall(tgt or \"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def _backoff_sleep(attempt: int, base: float = 0.5, jitter: float = 0.3):\n",
    "    time.sleep(base * (2 ** attempt) + random.random() * jitter)\n",
    "\n",
    "\n",
    "def _extract_translation(raw_text: str) -> str:\n",
    "    m = re.search(r\"<translation>([\\s\\S]*?)</translation>\", raw_text or \"\")\n",
    "    return (m.group(1).strip() if m else (raw_text or \"\")).strip()\n",
    "\n",
    "\n",
    "def translate_one(provider: str, cfg: Dict[str, str], src_text: str, target_lang: str) -> str:\n",
    "    header = PROMPT_HEADERS[target_lang]\n",
    "    prompt = f\"{header}{src_text}\\n\\n<translation>\"\n",
    "\n",
    "    for attempt in range(MAX_RETRIES):\n",
    "        try:\n",
    "            if provider == \"google\":\n",
    "                # New google.genai client\n",
    "                if hasattr(GOOGLE_CLIENT, \"models\"):\n",
    "                    resp = GOOGLE_CLIENT.models.generate_content(model=cfg[\"model\"], contents=prompt)\n",
    "                    raw = getattr(resp, \"text\", \"\") or \"\"\n",
    "                # Legacy google.generativeai\n",
    "                else:\n",
    "                    model = GOOGLE_CLIENT.GenerativeModel(cfg[\"model\"])  # type: ignore[attr-defined]\n",
    "                    resp = model.generate_content(prompt)\n",
    "                    raw = \"\".join([c.text for c in getattr(resp, \"candidates\", []) if hasattr(c, \"text\")]) or getattr(resp, \"text\", \"\") or \"\"\n",
    "                return _extract_translation(raw)\n",
    "\n",
    "            elif provider == \"openai\":\n",
    "                resp = OPENAI_CLIENT.chat.completions.create(\n",
    "                    model=cfg[\"model\"],\n",
    "                    messages=[\n",
    "                        {\"role\": \"system\", \"content\": \"You are a professional translator. Follow the examples and output only the translation enclosed in <translation> tags.\"},\n",
    "                        {\"role\": \"user\", \"content\": prompt},\n",
    "                    ],\n",
    "                    temperature=0.2,\n",
    "                    max_tokens=2048,\n",
    "                )\n",
    "                raw = resp.choices[0].message.content or \"\"\n",
    "                return _extract_translation(raw)\n",
    "\n",
    "            elif provider == \"anthropic\":\n",
    "                resp = ANTHROPIC_CLIENT.messages.create(\n",
    "                    model=cfg[\"model\"],\n",
    "                    max_tokens=2048,\n",
    "                    temperature=0.2,\n",
    "                    messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "                )\n",
    "                raw = \"\".join(getattr(b, \"text\", \"\") for b in resp.content).strip()\n",
    "                return _extract_translation(raw)\n",
    "\n",
    "            else:\n",
    "                raise ValueError(f\"Unknown provider: {provider}\")\n",
    "\n",
    "        except Exception as e:\n",
    "            # Retry on rate limit / transient failures\n",
    "            if attempt == MAX_RETRIES - 1:\n",
    "                print(f\"Final failure for provider={provider}: {e}\")\n",
    "                return \"[TRANSLATION_ERROR]\"\n",
    "            _backoff_sleep(attempt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "import os, json, time\n",
    "from pathlib import Path\n",
    "from typing import Dict, List, Tuple, Any\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Edited to fix the Claude overload issue\n",
    "PROVIDER_MAX_WORKERS = {\n",
    "    \"openai\":    int(os.getenv(\"OPENAI_MAX_WORKERS\", \"4\")),\n",
    "    \"anthropic\": int(os.getenv(\"ANTHROPIC_MAX_WORKERS\", \"1\")),  # Claude: keep low\n",
    "    \"google\":    int(os.getenv(\"GOOGLE_MAX_WORKERS\", \"2\")),\n",
    "}\n",
    "def _get_max_workers(provider: str, default_workers: int) -> int:\n",
    "    return max(1, PROVIDER_MAX_WORKERS.get(provider, default_workers))\n",
    "\n",
    "def run_translation_batch(model_key: str, provider: str, cfg: Dict[str, str], target_lang: str,\n",
    "                          unique_sources: List[str], src_to_paths: Dict[str, List[str]]\n",
    "                          ) -> Tuple[List[Dict], float]:\n",
    "    start = time.time()\n",
    "\n",
    "    out_dir = OUT_BASE / model_key\n",
    "    out_dir.mkdir(parents=True, exist_ok=True)\n",
    "    out_file = out_dir / f\"{target_lang}.json\"\n",
    "\n",
    "    existing_map: Dict[str, str] = {}\n",
    "    if out_file.exists():\n",
    "        try:\n",
    "            prev = json.load(open(out_file, \"r\", encoding=\"utf-8\"))\n",
    "            by_src: Dict[str, str] = {}\n",
    "            for row in prev:\n",
    "                # first translation per unique \"source\" wins (consistent with your format)\n",
    "                if \"source\" in row and \"translation\" in row and row[\"source\"] not in by_src:\n",
    "                    by_src[row[\"source\"]] = row[\"translation\"]\n",
    "            existing_map = by_src\n",
    "            print(f\"Resuming: found {len(existing_map)} prior unique translations\")\n",
    "        except Exception as e:\n",
    "            print(f\"  warn: failed to parse previous output ({out_file}): {e}\")\n",
    "\n",
    "    todo = [s for s in unique_sources if s not in existing_map]\n",
    "    results_map: Dict[str, str] = dict(existing_map)\n",
    "\n",
    "    # Provider-aware workers + gentle submit delay for Anthropic\n",
    "    max_workers = _get_max_workers(provider, default_workers=MAX_WORKERS)\n",
    "    submit_delay = 0.05 if provider == \"anthropic\" else 0.0\n",
    "\n",
    "    print(f\"Translating {len(todo)} unique strings with {model_key} → {target_lang}… (workers={max_workers})\")\n",
    "\n",
    "    if len(todo) > 0:\n",
    "        if max_workers == 1:\n",
    "            # Sequential path (stable for Claude)\n",
    "            for src in tqdm(todo, total=len(todo), desc=f\"{model_key} → {target_lang} (seq)\"):\n",
    "                try:\n",
    "                    results_map[src] = translate_one(provider, cfg, src, target_lang)\n",
    "                except Exception as e:\n",
    "                    print(f\"    warn: failed src≈{src[:48]!r}… | {e}\")\n",
    "                    results_map[src] = \"[TRANSLATION_ERROR]\"\n",
    "                if submit_delay:\n",
    "                    time.sleep(submit_delay)\n",
    "        else:\n",
    "            from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "            with ThreadPoolExecutor(max_workers=max_workers) as ex:\n",
    "                futures = {}\n",
    "                for src in todo:\n",
    "                    fut = ex.submit(translate_one, provider, cfg, src, target_lang)\n",
    "                    futures[fut] = src\n",
    "                    if submit_delay:\n",
    "                        time.sleep(submit_delay)  # soften burst for Anthropic\n",
    "                for fut in tqdm(as_completed(futures), total=len(futures), desc=f\"{model_key} → {target_lang}\"):\n",
    "                    src = futures[fut]\n",
    "                    try:\n",
    "                        results_map[src] = fut.result()\n",
    "                    except Exception as e:\n",
    "                        print(f\"    warn: failed src≈{src[:48]!r}… | {e}\")\n",
    "                        results_map[src] = \"[TRANSLATION_ERROR]\"\n",
    "\n",
    "    duration = time.time() - start\n",
    "\n",
    "    now = time.strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "    results: List[Dict[str, Any]] = []\n",
    "    for src, paths in src_to_paths.items():\n",
    "        tr = results_map.get(src, \"[TRANSLATION_ERROR]\")\n",
    "        for path in paths:\n",
    "            results.append({\n",
    "                \"path\": path,\n",
    "                \"source\": src,\n",
    "                \"translation\": tr,\n",
    "                \"model\": model_key,\n",
    "                \"target_lang\": target_lang,\n",
    "                \"timestamp\": now,\n",
    "            })\n",
    "\n",
    "    with open(out_file, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(results, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "    print(f\"{model_key} → {target_lang}: {len(results)} segments in {duration:.1f}s | saved: {out_file}\")\n",
    "    return results, duration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Metrics Helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def estimate_tokens(text: str) -> int:\n",
    "    # rough approximation: 4 chars/token\n",
    "    return max(1, len(text) // 4)\n",
    "\n",
    "\n",
    "def estimate_cost(results: List[Dict], model_key: str) -> Optional[float]:\n",
    "    if model_key not in PRICING_PER_1M_TOKENS:\n",
    "        return None\n",
    "    pricing = PRICING_PER_1M_TOKENS[model_key]\n",
    "    in_tok = sum(estimate_tokens(r[\"source\"]) for r in results)\n",
    "    out_tok = sum(estimate_tokens(r[\"translation\"]) for r in results)\n",
    "    cost = (in_tok / 1000000) * pricing[\"input\"] + (out_tok / 1000000) * pricing[\"output\"]\n",
    "    return round(cost, 4)\n",
    "\n",
    "\n",
    "def calculate_length_ratio(src: str, tgt: str) -> float:\n",
    "    src_len = max(len(src or \"\"), 1)\n",
    "    return len(tgt or \"\") / src_len\n",
    "\n",
    "\n",
    "def evaluate_translation_results(results: List[Dict], target_lang: str, model_key: str, duration: float) -> Dict:\n",
    "    n = len(results)\n",
    "    tag_ok = [tags_preserved(r[\"source\"], r[\"translation\"]) for r in results]\n",
    "    ratios = [calculate_length_ratio(r[\"source\"], r[\"translation\"]) for r in results]\n",
    "    errors = sum(1 for r in results if \"[TRANSLATION_ERROR]\" in r[\"translation\"])\n",
    "    avg_latency = duration / n if n else 0.0\n",
    "\n",
    "    metrics = {\n",
    "        \"model\": model_key,\n",
    "        \"target_language\": target_lang,\n",
    "        \"n_segments\": n,\n",
    "        \"error_count\": errors,\n",
    "        \"error_rate\": round(errors / n, 3) if n else 0,\n",
    "        \"tag_preservation_rate\": round(float(np.mean(tag_ok)), 3) if n else 0,\n",
    "        \"length_ratio_avg\": round(float(np.mean(ratios)), 3) if n else 0,\n",
    "        \"length_ratio_std\": round(float(np.std(ratios)), 3) if n else 0,\n",
    "        \"total_duration_sec\": round(duration, 2),\n",
    "        \"avg_latency_sec\": round(avg_latency, 3),\n",
    "        \"segments_per_minute\": round(n / (duration / 60), 1) if duration > 0 else 0,\n",
    "        \"estimated_cost_usd\": estimate_cost(results, model_key),\n",
    "        \"few_shot_prompting\": True,\n",
    "        \"evaluation_timestamp\": time.strftime(\"%Y-%m-%d %H:%M:%S\"),\n",
    "    }\n",
    "    return metrics\n",
    "\n",
    "\n",
    "def save_metrics(metrics: Dict, model_key: str, target_lang: str):\n",
    "    out_dir = EVAL_BASE / model_key\n",
    "    out_dir.mkdir(parents=True, exist_ok=True)\n",
    "    out_path = out_dir / f\"metrics_{target_lang}.json\"\n",
    "    with open(out_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(metrics, f, ensure_ascii=False, indent=2)\n",
    "    print(f\"Saved metrics: {out_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Main Translation + Evaluation Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Starting translation + evaluation…\n",
      "========================================================================\n",
      "\n",
      "▶ Model: gemini-2.5-flash (google)\n",
      "  → Language: French (fr)\n",
      "Translating 73 unique strings with gemini-2.5-flash → fr… (workers=2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "gemini-2.5-flash → fr:  16%|█████████▋                                                 | 12/73 [00:26<01:51,  1.83s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final failure for provider=google: 429 RESOURCE_EXHAUSTED. {'error': {'code': 429, 'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits.\\n* Quota exceeded for metric: generativelanguage.googleapis.com/generate_content_free_tier_requests, limit: 10\\nPlease retry in 6.011507153s.', 'status': 'RESOURCE_EXHAUSTED', 'details': [{'@type': 'type.googleapis.com/google.rpc.QuotaFailure', 'violations': [{'quotaMetric': 'generativelanguage.googleapis.com/generate_content_free_tier_requests', 'quotaId': 'GenerateRequestsPerMinutePerProjectPerModel-FreeTier', 'quotaDimensions': {'location': 'global', 'model': 'gemini-2.5-flash'}, 'quotaValue': '10'}]}, {'@type': 'type.googleapis.com/google.rpc.Help', 'links': [{'description': 'Learn more about Gemini API quotas', 'url': 'https://ai.google.dev/gemini-api/docs/rate-limits'}]}, {'@type': 'type.googleapis.com/google.rpc.RetryInfo', 'retryDelay': '6s'}]}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "gemini-2.5-flash → fr:  19%|███████████▎                                               | 14/73 [00:29<01:21,  1.38s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final failure for provider=google: 429 RESOURCE_EXHAUSTED. {'error': {'code': 429, 'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits.\\n* Quota exceeded for metric: generativelanguage.googleapis.com/generate_content_free_tier_requests, limit: 10\\nPlease retry in 3.704801524s.', 'status': 'RESOURCE_EXHAUSTED', 'details': [{'@type': 'type.googleapis.com/google.rpc.QuotaFailure', 'violations': [{'quotaMetric': 'generativelanguage.googleapis.com/generate_content_free_tier_requests', 'quotaId': 'GenerateRequestsPerMinutePerProjectPerModel-FreeTier', 'quotaDimensions': {'model': 'gemini-2.5-flash', 'location': 'global'}, 'quotaValue': '10'}]}, {'@type': 'type.googleapis.com/google.rpc.Help', 'links': [{'description': 'Learn more about Gemini API quotas', 'url': 'https://ai.google.dev/gemini-api/docs/rate-limits'}]}, {'@type': 'type.googleapis.com/google.rpc.RetryInfo', 'retryDelay': '3s'}]}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "gemini-2.5-flash → fr: 100%|███████████████████████████████████████████████████████████| 73/73 [02:06<00:00,  1.73s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gemini-2.5-flash → fr: 76 segments in 126.1s | saved: translations\\baseline\\gemini-2.5-flash\\fr.json\n",
      "Saved metrics: eval\\baseline\\gemini-2.5-flash\\metrics_fr.json\n",
      "    Key metrics — Tag: 98.7% | Latency: 1.66s | Speed: 36.2 seg/min | Cost: $0.0004\n",
      "  → Language: Japanese (ja)\n",
      "Translating 73 unique strings with gemini-2.5-flash → ja… (workers=2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "gemini-2.5-flash → ja: 100%|███████████████████████████████████████████████████████████| 73/73 [03:21<00:00,  2.76s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gemini-2.5-flash → ja: 76 segments in 201.3s | saved: translations\\baseline\\gemini-2.5-flash\\ja.json\n",
      "Saved metrics: eval\\baseline\\gemini-2.5-flash\\metrics_ja.json\n",
      "    Key metrics — Tag: 97.4% | Latency: 2.65s | Speed: 22.7 seg/min | Cost: $0.0003\n",
      "  → Language: Italian (it)\n",
      "Translating 73 unique strings with gemini-2.5-flash → it… (workers=2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "gemini-2.5-flash → it: 100%|███████████████████████████████████████████████████████████| 73/73 [02:31<00:00,  2.08s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gemini-2.5-flash → it: 76 segments in 151.8s | saved: translations\\baseline\\gemini-2.5-flash\\it.json\n",
      "Saved metrics: eval\\baseline\\gemini-2.5-flash\\metrics_it.json\n",
      "    Key metrics — Tag: 100.0% | Latency: 2.00s | Speed: 30.0 seg/min | Cost: $0.0004\n",
      "\n",
      "▶ Model: gpt-4o-mini (openai)\n",
      "  → Language: French (fr)\n",
      "Translating 73 unique strings with gpt-4o-mini → fr… (workers=4)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "gpt-4o-mini → fr: 100%|████████████████████████████████████████████████████████████████| 73/73 [00:32<00:00,  2.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gpt-4o-mini → fr: 76 segments in 32.7s | saved: translations\\baseline\\gpt-4o-mini\\fr.json\n",
      "Saved metrics: eval\\baseline\\gpt-4o-mini\\metrics_fr.json\n",
      "    Key metrics — Tag: 100.0% | Latency: 0.43s | Speed: 139.5 seg/min | Cost: $0.0008\n",
      "  → Language: Japanese (ja)\n",
      "Translating 73 unique strings with gpt-4o-mini → ja… (workers=4)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "gpt-4o-mini → ja: 100%|████████████████████████████████████████████████████████████████| 73/73 [00:22<00:00,  3.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gpt-4o-mini → ja: 76 segments in 22.8s | saved: translations\\baseline\\gpt-4o-mini\\ja.json\n",
      "Saved metrics: eval\\baseline\\gpt-4o-mini\\metrics_ja.json\n",
      "    Key metrics — Tag: 100.0% | Latency: 0.30s | Speed: 199.9 seg/min | Cost: $0.0005\n",
      "  → Language: Italian (it)\n",
      "Translating 73 unique strings with gpt-4o-mini → it… (workers=4)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "gpt-4o-mini → it: 100%|████████████████████████████████████████████████████████████████| 73/73 [00:21<00:00,  3.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gpt-4o-mini → it: 76 segments in 21.9s | saved: translations\\baseline\\gpt-4o-mini\\it.json\n",
      "Saved metrics: eval\\baseline\\gpt-4o-mini\\metrics_it.json\n",
      "    Key metrics — Tag: 100.0% | Latency: 0.29s | Speed: 208.1 seg/min | Cost: $0.0008\n",
      "\n",
      "▶ Model: claude-3-5-sonnet (anthropic)\n",
      "  → Language: French (fr)\n",
      "Translating 73 unique strings with claude-3-5-sonnet → fr… (workers=1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "claude-3-5-sonnet → fr (seq): 100%|████████████████████████████████████████████████████| 73/73 [01:13<00:00,  1.01s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "claude-3-5-sonnet → fr: 76 segments in 73.9s | saved: translations\\baseline\\claude-3-5-sonnet\\fr.json\n",
      "Saved metrics: eval\\baseline\\claude-3-5-sonnet\\metrics_fr.json\n",
      "    Key metrics — Tag: 17.1% | Latency: 0.97s | Speed: 61.7 seg/min | Cost: $0.0236\n",
      "  → Language: Japanese (ja)\n",
      "Translating 73 unique strings with claude-3-5-sonnet → ja… (workers=1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "claude-3-5-sonnet → ja (seq): 100%|████████████████████████████████████████████████████| 73/73 [01:52<00:00,  1.54s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "claude-3-5-sonnet → ja: 76 segments in 112.6s | saved: translations\\baseline\\claude-3-5-sonnet\\ja.json\n",
      "Saved metrics: eval\\baseline\\claude-3-5-sonnet\\metrics_ja.json\n",
      "    Key metrics — Tag: 32.9% | Latency: 1.48s | Speed: 40.5 seg/min | Cost: $0.0151\n",
      "  → Language: Italian (it)\n",
      "Translating 73 unique strings with claude-3-5-sonnet → it… (workers=1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "claude-3-5-sonnet → it (seq): 100%|████████████████████████████████████████████████████| 73/73 [01:38<00:00,  1.35s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "claude-3-5-sonnet → it: 76 segments in 98.3s | saved: translations\\baseline\\claude-3-5-sonnet\\it.json\n",
      "Saved metrics: eval\\baseline\\claude-3-5-sonnet\\metrics_it.json\n",
      "    Key metrics — Tag: 18.4% | Latency: 1.29s | Speed: 46.4 seg/min | Cost: $0.0230\n",
      "\n",
      "Pipeline done. Model-language combos processed: 9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "#Begin the LLM Battle Royale\n",
    "\n",
    "print(\"\\nStarting translation + evaluation…\")\n",
    "print(\"=\" * 72)\n",
    "\n",
    "all_results: List[Dict] = []\n",
    "all_metrics: List[Dict] = []\n",
    "\n",
    "for model_key, (provider, cfg) in BASELINE_MODELS.items():\n",
    "    print(f\"\\n▶ Model: {model_key} ({provider})\")\n",
    "    for lang in TARGET_LANGUAGES:\n",
    "        print(f\"  → Language: {LANGUAGE_NAMES[lang]} ({lang})\")\n",
    "        try:\n",
    "            results, duration = run_translation_batch(model_key, provider, cfg, lang, UNIQUE_SOURCES, SRC_TO_PATHS)\n",
    "            metrics = evaluate_translation_results(results, lang, model_key, duration)\n",
    "            save_metrics(metrics, model_key, lang)\n",
    "            all_results.extend(results)\n",
    "            all_metrics.append(metrics)\n",
    "            print(\n",
    "                \"    Key metrics — \"\n",
    "                f\"Tag: {metrics['tag_preservation_rate']:.1%} | \"\n",
    "                f\"Latency: {metrics['avg_latency_sec']:.2f}s | \"\n",
    "                f\"Speed: {metrics['segments_per_minute']:.1f} seg/min | \"\n",
    "                f\"Cost: {('$' + format(metrics['estimated_cost_usd'], '.4f')) if metrics['estimated_cost_usd'] is not None else 'n/a'}\"\n",
    "            )\n",
    "        except Exception as e:\n",
    "            print(f\"Failed {model_key} → {lang}: {e}\")\n",
    "            continue\n",
    "\n",
    "print(f\"\\nPipeline done. Model-language combos processed: {len(all_metrics)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Comparison Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "TRANSLATION QUALITY COMPARISON\n",
      "========================================================================\n",
      "\n",
      "French (FR)\n",
      "----------------------------------------\n",
      "Model: gemini-2.5-flash\n",
      "  Tag preservation: 98.7%\n",
      "  Avg latency: 1.66s\n",
      "  Speed: 36.2 seg/min\n",
      "  Estimated cost: $0.0004\n",
      "\n",
      "Model: gpt-4o-mini\n",
      "  Tag preservation: 100.0%\n",
      "  Avg latency: 0.43s\n",
      "  Speed: 139.5 seg/min\n",
      "  Estimated cost: $0.0008\n",
      "\n",
      "Model: claude-3-5-sonnet\n",
      "  Tag preservation: 17.1%\n",
      "  Avg latency: 0.97s\n",
      "  Speed: 61.7 seg/min\n",
      "  Estimated cost: $0.0236\n",
      "\n",
      "\n",
      "Japanese (JA)\n",
      "----------------------------------------\n",
      "Model: gemini-2.5-flash\n",
      "  Tag preservation: 97.4%\n",
      "  Avg latency: 2.65s\n",
      "  Speed: 22.7 seg/min\n",
      "  Estimated cost: $0.0003\n",
      "\n",
      "Model: gpt-4o-mini\n",
      "  Tag preservation: 100.0%\n",
      "  Avg latency: 0.30s\n",
      "  Speed: 199.9 seg/min\n",
      "  Estimated cost: $0.0005\n",
      "\n",
      "Model: claude-3-5-sonnet\n",
      "  Tag preservation: 32.9%\n",
      "  Avg latency: 1.48s\n",
      "  Speed: 40.5 seg/min\n",
      "  Estimated cost: $0.0151\n",
      "\n",
      "\n",
      "Italian (IT)\n",
      "----------------------------------------\n",
      "Model: gemini-2.5-flash\n",
      "  Tag preservation: 100.0%\n",
      "  Avg latency: 2.00s\n",
      "  Speed: 30.0 seg/min\n",
      "  Estimated cost: $0.0004\n",
      "\n",
      "Model: gpt-4o-mini\n",
      "  Tag preservation: 100.0%\n",
      "  Avg latency: 0.29s\n",
      "  Speed: 208.1 seg/min\n",
      "  Estimated cost: $0.0008\n",
      "\n",
      "Model: claude-3-5-sonnet\n",
      "  Tag preservation: 18.4%\n",
      "  Avg latency: 1.29s\n",
      "  Speed: 46.4 seg/min\n",
      "  Estimated cost: $0.0230\n",
      "\n",
      "Comparison saved: eval\\baseline\\comparison_results.csv\n"
     ]
    }
   ],
   "source": [
    "if all_metrics:\n",
    "    comparison_df = pd.DataFrame(all_metrics)\n",
    "    print(\"\\nTRANSLATION QUALITY COMPARISON\")\n",
    "    print(\"=\" * 72)\n",
    "    for lang in TARGET_LANGUAGES:\n",
    "        sub = comparison_df[comparison_df[\"target_language\"] == lang]\n",
    "        if not sub.empty:\n",
    "            print(f\"\\n{LANGUAGE_NAMES[lang]} ({lang.upper()})\")\n",
    "            print(\"-\" * 40)\n",
    "            for _, row in sub.iterrows():\n",
    "                cost = (f\"${row['estimated_cost_usd']:.4f}\" if pd.notna(row['estimated_cost_usd']) else \"n/a\")\n",
    "                print(\n",
    "                    f\"Model: {row['model']}\\n\"\n",
    "                    f\"  Tag preservation: {row['tag_preservation_rate']:.1%}\\n\"\n",
    "                    f\"  Avg latency: {row['avg_latency_sec']:.2f}s\\n\"\n",
    "                    f\"  Speed: {row['segments_per_minute']:.1f} seg/min\\n\"\n",
    "                    f\"  Estimated cost: {cost}\\n\"\n",
    "                )\n",
    "    comp_path = EVAL_BASE / \"comparison_results.csv\"\n",
    "    comparison_df.to_csv(comp_path, index=False, encoding=\"utf-8\")\n",
    "    print(f\"Comparison saved: {comp_path}\")\n",
    "else:\n",
    "    print(\"No metrics available for comparison\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
